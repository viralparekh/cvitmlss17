{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Generative Adversarial Networks\n",
    "\n",
    "In this lab, we will explore several architectures of generative adversarial networks. There are many different versions of GANs existing today. \n",
    "\n",
    "![](Gans.jpg)\n",
    "\n",
    "The break-up for today's lab is as follows:\n",
    "<ol>\n",
    "<li>Vanilla GAN (DCGAN)</li>\n",
    "<li>Conditional GAN (CGAN)</li>\n",
    "<li>Auxiliary Classifier GAN (AC-GAN) </li>\n",
    "<li>Bi-directional GAN (BiGAN) </li>\n",
    "</ol>\n",
    "\n",
    "These codes are modified from https://github.com/wiseodd/generative-models/tree/master/GAN\n",
    "\n",
    "## Module 1: Vanilla GAN (using DCGANs)\n",
    "\n",
    "Let us first look at a normal generative adversarial network. \n",
    "\n",
    "First, let us define some constants and import some libraries. We also define a custom weights initialization function.\n",
    "\n",
    "(Read later about Xavier initialization: http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)\n",
    "(Link to paper: https://arxiv.org/abs/1406.2661)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    vec = torch.randn(*size) * xavier_stddev\n",
    "    if setcuda:\n",
    "        vec = vec.cuda()\n",
    "    return Variable(vec, requires_grad=True)\n",
    "\n",
    "setcuda = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using MNIST dataset for these experiments. Let us load the dataset. We will also define some functions that will be used for the other modules of this lab. Note that we are flattening the MNIST images into one dimensional vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0,), (1,))])\n",
    "trainset = torchvision.datasets.MNIST(root='../../data/lab3', train=True,\n",
    "                                        download=False,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')\n",
    "\n",
    "def onehotencoder(x):\n",
    "    y = np.zeros((x.numpy().shape[0],10))\n",
    "    for i in range(x.numpy().shape[0]):\n",
    "        y[i,x[i]] = 1\n",
    "    return y\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "def mnist_next(dataiter):\n",
    "\n",
    "    try:\n",
    "        images, labels = dataiter.next()\n",
    "        labels = onehotencoder(labels)\n",
    "        images = images.view(images.numpy().shape[0],28*28)\n",
    "    except:\n",
    "        dataiter = iter(trainloader)\n",
    "        images, labels = dataiter.next()\n",
    "        labels = onehotencoder(labels)\n",
    "        images = images.view(images.numpy().shape[0],28*28)\n",
    "    return images.numpy(), labels\n",
    "\n",
    "def initialize_loader(trainset):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    dataiter = iter(trainloader)\n",
    "    return dataiter\n",
    "\n",
    "images, labels = mnist_next(dataiter)\n",
    "# print some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "img = torchvision.utils.make_grid(images)\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the network structure. For this experiment, we are not using deep networks. We are not even using torch.nn layers. Instead, we will use simple linear fully connected layers. The generator and discriminators are 2-layer networks. This is why we flattened the images in the block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bzvar = torch.zeros(h_dim);\n",
    "bhvar = torch.zeros(X_dim);\n",
    "if setcuda:\n",
    "    bzvar = bzvar.cuda()\n",
    "    bhvar = bhvar.cuda()\n",
    "    \n",
    "bzh = Variable(bzvar, requires_grad=True)\n",
    "bhx = Variable(bhvar, requires_grad=True)\n",
    "\n",
    "def G(z):\n",
    "    h = nn.relu(torch.mm(z, Wzh) + bzh.repeat(z.size(0), 1))\n",
    "    X = nn.sigmoid(torch.mm(h, Whx) + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bxvar = torch.zeros(h_dim)\n",
    "bhvar = torch.zeros(1)\n",
    "\n",
    "if setcuda:\n",
    "    bxvar = bxvar.cuda()\n",
    "    bhvar = bhvar.cuda()\n",
    "    \n",
    "bxh = Variable(bxvar, requires_grad=True)\n",
    "bhy = Variable(bhvar, requires_grad=True)\n",
    "\n",
    "def D(X):\n",
    "    h = nn.relu(torch.mm(X, Wxh) + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(torch.mm(h, Why) + bhy.repeat(h.size(0), 1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will gather the parameters of the generator and the discriminator so that they can be given to the Adam optimizer to update the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))\n",
    "\n",
    "if setcuda:\n",
    "    ones_label = ones_label.cuda()\n",
    "    zeros_label = zeros_label.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will start the actual training. The training alternates between updating the discriminator network's weights and updating the generator's weight.\n",
    "\n",
    "First, we update the discriminator's weight. We take a minibatch from the dataset and do a forward pass on the discriminator with the label '1'. Then, we feed noise into the generator and feed the generated images into the discriminator with the label '0'. We backpropagate the error and update the discriminator weights. \n",
    "\n",
    "To update the generator weights, we feed noise to the generator and feed the generated images into the discriminator with the label '1'. This error is backpropagated to update teh weights of G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = initialize_loader(trainset)\n",
    "\n",
    "for it in range(1000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    X, _ = mnist_next(dataiter)\n",
    "    if X.shape[0]!=mb_size:\n",
    "        dataiter = initialize_loader(trainset)\n",
    "        X,_ = mnist_next(dataiter)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "    \n",
    "    if setcuda:\n",
    "        X = X.cuda()\n",
    "        z = z.cuda()\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    if setcuda:\n",
    "        z = Variable(torch.randn(mb_size, Z_dim).cuda())\n",
    "    else:\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.cpu().numpy(), G_loss.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the images generated by the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(mb_size, Z_dim))\n",
    "if setcuda:\n",
    "    z = z.cuda()\n",
    "samples = G(z)\n",
    "samples = samples.cpu()\n",
    "img = samples.data\n",
    "img = img.view([64,1,28,28])\n",
    "img = torchvision.utils.make_grid(img)\n",
    "img = img.permute(1,2,0)\n",
    "plt.imshow(img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions/Exercises\n",
    "<ol>\n",
    "<li> How long do we have to train the GAN to get good results? Can you plot the loss of the generator and discriminator and see if there is a correlation?</li>\n",
    "<li> What is the correlation between the input random noise and the output produced by the generator? Can you vary the input noise a little and see the result on the generated images? </li>\n",
    "<li> Is there any way to determine the class of the generated images by changing the input noise vector?</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
