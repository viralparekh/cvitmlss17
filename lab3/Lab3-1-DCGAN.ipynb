{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Generative Adversarial Networks\n",
    "\n",
    "In this lab, we will explore several architectures of generative adversarial networks. The break-up is as follows:\n",
    "<ol>\n",
    "<li>Vanilla GAN (DCGAN)</li>\n",
    "<li>Conditional GAN (CGAN)</li>\n",
    "<li>Auxiliary Classifier GAN (AC-GAN) </li>\n",
    "<li>Bi-directional GAN (BiGAN) </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "## Module 1: Vanilla GAN (using DCGANs)\n",
    "\n",
    "Let us first look at a normal generative adversarial network, using the structure known as DCGAN. \n",
    "\n",
    "First, let us define some constants and import some libraries. We also define a custom weights initialization function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "\n",
    "# let us define some parameters\n",
    "workers = 2\n",
    "batchSize=4\n",
    "nz=10\n",
    "ngf = 64\n",
    "ndf=64\n",
    "niter=1 #number of epochs\n",
    "lr=0.0002 #learning rate for adam\n",
    "beta1=0.5 #beta1 for adam\n",
    "cuda=True\n",
    "ngpu=1 #number of gpus to use\n",
    "outf = './output'\n",
    "manualSeed = 67\n",
    "nc = 1 #number of output channels\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GAN architecture consists of a generator network, and a discriminator network, trained in an alternating manner. The generator network takes in noise and generates an image. The discriminator network takes in an image and determines whether the image is produced by the generator or taken from the database.\n",
    "\n",
    "Let us define the discriminator network. In keeping with the principles put forward in the DCGAN paper, we use LeakyReLUs as the non-linearities. We will also define a criterion which will calculate the binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1)\n",
    "    \n",
    "netD = _netD()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the generator network and create an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "             nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    \n",
    "netG = _netG()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ADAM optimizer to train the networks. Let us set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the sort of images that are generated by an untrained generator network using the random weight initialization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nbatchSize = 64\n",
    "noise = torch.FloatTensor(nbatchSize, nz,1,1)\n",
    "noise.normal_(0,1)\n",
    "noise = Variable(noise)\n",
    "if cuda:\n",
    "    noise = noise.cuda()\n",
    "output = netG.forward(input=noise)\n",
    "print(output.size())\n",
    "\n",
    "output = output.cpu()\n",
    "output = output.data\n",
    "output = torchvision.utils.make_grid(output)\n",
    "output = output.permute(1,2,0)\n",
    "plt.imshow(output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will put the networks to the GPU so training can happen faster. Please note, if the networks are on GPU, we should put all the data we use with the networks on GPU too, and vice versa. Otherwise, it will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(batchSize, 1, 64, 64)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#cuda=False\n",
    "if cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will use the MNIST dataset to train the GAN. The package torchvision provides some methods for us to easily use the MNIST dataset and some other commonly used datasets. Let us load the dataset now. We will resize the images from 28x28 to 64x64 as required by the networks we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transformation to resize the image to 64x64\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Scale(64),\n",
    "     transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "     transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# load the dataset\n",
    "trainset = torchvision.datasets.MNIST(root='../../data/lab3', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')\n",
    "\n",
    "# print some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "img = torchvision.utils.make_grid(images)\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the network. The code below runs for a 100 batches. You run this block again and again, or change the number of iterations to train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        if i>1000:\n",
    "            print('done 100 iterations')\n",
    "            break\n",
    "        if i%100==0:\n",
    "            print(i, end=' ')\n",
    "            \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if cuda:\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        inputv = Variable(input)\n",
    "        labelv = Variable(label)\n",
    "\n",
    "        output = netD(inputv)\n",
    "        errD_real = criterion(output, labelv)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # display some generated images for each 200 iterations\n",
    "        if i%100 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            fake = fake.data\n",
    "            fake = fake[0:16,:,:,:]\n",
    "            fake = fake.cpu()\n",
    "            fake = torchvision.utils.make_grid(fake)\n",
    "            fake = fake.permute(1,2,0)\n",
    "            plt.imshow(fake.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back to the display module we made earlier and run it to see some of the images the generator creates once it is trained.\n",
    "\n",
    "### Questions/Exercises\n",
    "<ol>\n",
    "<li> How long do we have to train the DCGAN to get good results? Can you plot the loss of the generator and discriminator and see if there is a correlation? (Hint: loss of generator is errG, loss of discriminator is errD)</li>\n",
    "<li> What is the correlation between the input random noise and the output produced by the generator? Can you vary the input noise a little and see the result on the generated images? </li>\n",
    "<li> Is there any way to determine the class of the generated images by changing the input noise vector?</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
