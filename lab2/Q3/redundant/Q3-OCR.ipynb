{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training an OCR using RNN + CTC on Synthetic Images ##\n",
    "- To train neural network to do seq2seq mapping, when your input sequence and output sequence are not aligned\n",
    "- Input sequence is a sequence of image features and output is a sequence of characters\n",
    "- Images are resized to  fixed width , though we can have variying widths since RNN can handle variable length sequences. This helps in faster batch learning\n",
    "- Training images are rendered on the fly for the task\n",
    "- The network is tested on synthetic images, but rendered from out-of-vocabulary words\n",
    "- We train a network with a bidirectional RNN  and a CTC loss for the task\n",
    "     - A word image's each column is treated as a timestep. so inputdim= height of the word image and seqlen= width of the image\n",
    "     - Here to make sure the networks learns the mappings we first overfit it to 3 letter words\n",
    "     - Then we will the train network on a larger dataset, comprising of images rendered from 90k English words\n",
    "- Now we change the network architecture slightly ; we add a convolutional stack before the BLSTM layer\n",
    "    - Now your input to the network is not the raw pixel values, But we do steps of convolution and maxpooling and the resultant output is reshpaed to form a Time x featDim structured before it is fed to the network. \n",
    "    - The convoultional stack can be increased in depth to get better feature represenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Use a BRNN + CTC to recognize given word image \n",
    "# Network is trained on images rendered using PIL \n",
    "# ============================================================================\n",
    "# for ML Summer School 2017 at IIIT - HYD\n",
    "# Authors -minesh\n",
    "# Do not share this code or the associated exercises anywhere\n",
    "# we might be using the same code/ exercies for our future schools/ events\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import numpy as np\n",
    "import time,math\n",
    "from time import sleep\n",
    "import random\n",
    "import sys,codecs,glob \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "random.seed(0)\n",
    "# TODO - MAKE SURE CTC IS INSTALLED IN ALL MACHINES\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary and the fonts ####\n",
    "-  loading the lexicon of 90k words\n",
    "- get the fontslist to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the vocabulary = 88172\n",
      "number of fonts in the collection = 65\n"
     ]
    }
   ],
   "source": [
    "#all word images are resized to a height of 32 pixels\n",
    "imHeight=32 \n",
    "\"\"\"\n",
    "image width is also set a fixed size\n",
    "YES. Though RNNS can handle variable length sequences we resize them to fixed width\n",
    "This is for the ease of batch learning\n",
    "And it doesnt seem to affect the performance much atleast in our case\n",
    "\n",
    "Pytorch provides a packed array API incase you want to have variable length sequences within a batch\n",
    "see the discussion here\n",
    "https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120/8\n",
    "\n",
    "\"\"\"\n",
    "imWidth=100\n",
    "#imWidth=15\n",
    "#65 google fonts are used\n",
    "fontsList=glob.glob('../../../data/lab2/googleFonts/'+'*.ttf')\n",
    "#lexicon has 90k words\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "fontSizeOptions={'16','20','24','28','30','32','36','38'}\n",
    "\n",
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "#alphabet=\"(3)-\"\n",
    "dict={}\n",
    "for i, char in enumerate(alphabet):\n",
    "\tdict[char] = i + 1\n",
    "print('number of words in the vocabulary =', vocabSize)\n",
    "print('number of fonts in the collection =', len(fontsList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple helper function to compute time since some 'start time'\n",
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return the class labels for each character in the targetsequence \n",
    "def Str2Labels(text):\n",
    "\tglobal dict\n",
    "\ttext = [dict[char.lower()] for char in text]\n",
    "\t#print (text)\n",
    "\tlength=len(text)\n",
    "\treturn text, length\n",
    "#StrtoLabels(\"0-1\")\n",
    "\n",
    "### from the predicted sequence of labels for an image, decode the string\n",
    "# function returns the rawstring and also the decoded string after removing blanks and duplicates\n",
    "\n",
    "#eg: if labelsequnce you get after an argmax on the output activation matris is  [12,12,0,0,15,0,15,15,0,0]\n",
    "#then your raw label string would be \"bb~~e~ee~~\" and the outputstring \"bee\"\n",
    "def Labels2Str(predictedLabelSequences):\n",
    "    bz=predictedLabelSequences.size(0)\n",
    "    predictedRawStrings=[]\n",
    "    predictedStrings=[]\n",
    "    for i in range(0,bz):\n",
    "        predictedRawString=\"\"\n",
    "        predictedString=\"\"\n",
    "        predictedLabelSeq=predictedLabelSequences.data[i,:]\n",
    "        prevId=1000 #just a large value which is not in the index \n",
    "        character=\"\"\n",
    "        character_raw=\"\"\n",
    "        for j in range (0, predictedLabelSeq.size(0)):\n",
    "            idx=predictedLabelSeq[j]\n",
    "            if (prevId != 1000 or prevId!=idx) :\n",
    "                if prevId!=idx:\n",
    "                    if idx==0:\n",
    "                        character_raw=\"~\"\n",
    "                        character=\"\"\n",
    "                    else:\n",
    "                        character_raw=alphabet[idx-1]\n",
    "                        character=alphabet[idx-1]\n",
    "                else:\n",
    "                    character_raw=\"~\"\n",
    "                    character=\"\"\n",
    "                prevId=idx\n",
    "            else:\n",
    "                character=\"\"\n",
    "                if idx==0:\n",
    "                    character_raw=\"~\"\n",
    "                else:\n",
    "                    character_raw=alphabet[idx-1]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            predictedString+=character\n",
    "            predictedRawString+=character_raw\n",
    "        predictedRawStrings.append(predictedRawString)\n",
    "        predictedStrings.append(predictedString)\n",
    "        \n",
    "    return predictedRawStrings, predictedStrings\n",
    "\n",
    "\n",
    "\n",
    "def image2tensor(im):\n",
    "    #returns the pixel values of a PIL image (in 0-1 range) as a numpy 2D array\n",
    "\n",
    "    (width, height) = im.size\n",
    "    greyscale_map = list(im.getdata())\n",
    "    greyscale_map = np.array(greyscale_map, dtype = np.uint8)\n",
    "    greyscale_map=greyscale_map.astype(float)\n",
    "    greyscale_map = torch.from_numpy(greyscale_map.reshape((height, width))).float()/255.0\n",
    "    return greyscale_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images, prepare a training batch ###\n",
    "- renders a batch of word images, from the list of words supplied\n",
    "- if singleFont is true then only one font would be used to render images. This is useful in case where you want to test overfitting the network to easy examples\n",
    "- Along with the rendered images, the target strings are converted to corresponding sequence of labels; for example the word \"bee\" would be converted to [12,15,15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetBatch ( imWidth,batchOfWords,singleFont ):\n",
    "    \"\"\"\n",
    "    Renders a batch of word images and returns the images along with the corresponding GTs\n",
    "    Uses PIL to render word images\n",
    "    font is randomly picked from a set of freely available google fonts\n",
    "    word is picked from a vocabulary of English words\n",
    "\n",
    "    \"\"\"\n",
    "    wordImages=[]\n",
    "    labelSequences=[]\n",
    "    labelSeqLengths=[]\n",
    "\n",
    "    for  i,text in enumerate (batchOfWords):\n",
    "        wordText=text\n",
    "        if singleFont==1:\n",
    "            fontName=fontsList[0]\n",
    "            fontSize='26'\n",
    "        else:\n",
    "            fontName=random.sample(fontsList,1)[0]\n",
    "            fontSize=random.sample(fontSizeOptions,1)[0] \n",
    "        imageFont = ImageFont.truetype(fontName,int(fontSize))\n",
    "        textSize=imageFont.getsize(wordText)\n",
    "        img=Image.new(\"L\", textSize,(255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((0, 0),wordText,(0),font=imageFont)\n",
    "        img=img.resize((imWidth,imHeight), Image.ANTIALIAS)\n",
    "        #img.save(text+'.jpeg')\n",
    "\n",
    "        imgTensor=image2tensor(img)\n",
    "        imgTensor=imgTensor.unsqueeze(0) # at 0 a new dimenion is added\n",
    "\n",
    "        wordImages.append(imgTensor)\n",
    "\n",
    "        labelSeq,l=Str2Labels(wordText)\n",
    "        labelSequences+=labelSeq\n",
    "        labelSeqLengths.append(l)\n",
    "    \n",
    "    batchImageTensor=torch.cat(wordImages,0) # BxHxW\n",
    "    \n",
    "    #now all the image tensors are combined ( we  did the unsqueeze eariler for this cat)  \n",
    "    #print ('size of batchimage tensor before transpose', batchImageTensor.size())\n",
    "    #batchImageTensor=torch.transpose(batchImageTensor,1,2) # BxWxH\n",
    "    #print ('size of batchimage tensor after transpose', batchImageTensor.size())\n",
    "    labelSequencesTensor=torch.IntTensor(labelSequences)\n",
    "    labelSeqLengthsTensor=torch.IntTensor(labelSeqLengths)\n",
    "    return batchImageTensor, labelSequencesTensor, labelSeqLengthsTensor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defintion  ###\n",
    "![OCR Architecture](blstm.jpg)\n",
    "- Input image here is of shape 100*32. Hence seqLen=100 and your featDim at a timestep =32\n",
    "- The below network has two BLSTM layers with #neurons in each layer = hiddenDim\n",
    "- the outputs of both the forward and backward recurrent layers in the second hidden layer are connected to a linear layer. There are hiddenDim*2 connections coming to this layer and its output is of size=outputDim=nClasses+1 (one extra class for blank label of CTC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minesh TODO split blstm into a separate class ?\n",
    "\n",
    "class rnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(rnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        # bidirectional= true to make the rnn bidirectional\n",
    "        self.blstm1=nn.LSTM(inputDim, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "                \n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "                \n",
    "    def forward(self, x ):\n",
    "        #incoming x is of shape BxHXW\n",
    "        #we reshape it to BxWxH\n",
    "        x=x.transpose(1,2)\n",
    "        #print ('size of x=', x.size()) # BxWxH\n",
    "        B,T,D  = x.size(0), x.size(1), x.size(2)\n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "                \n",
    "        # output of RNN is reshaped to B*T x D before it is fed to the linear layer\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        # the activations are reshaped to B x T x outputDim size\n",
    "        #then a transpose of B and T since CTC expects the T to be first\n",
    "        outputLayerActivations= outputLayerActivations.view(B,T,-1).transpose(0,1)\n",
    "        #if use_cuda:\n",
    "        #    outputLayerActivations=outputLayerActivations.cuda()\n",
    "        return outputLayerActivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing the model and other hyper parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,singleFont, saveTrue ):\n",
    "    batchSize=40\n",
    "    nHidden=80\n",
    "    nClasses= len(alphabet)\n",
    "    criterion = CTCLoss()\n",
    "    numLayers=2# \n",
    "    numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "    model = rnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "        criterion=criterion.cuda()\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start = time.time()\n",
    "    for iter in range (0,200):\n",
    "        avgTrainCost=0\n",
    "        random.shuffle(words)\n",
    "        \n",
    "        for i in range (0,vocabSize-batchSize+1,batchSize):\n",
    "\n",
    "            model.zero_grad()\n",
    "            #words which need to be rendered into images are sequentially taken from the lexicon\n",
    "            #the number of words rendered at a time = batchSize\n",
    "            batchOfWords=words[i:i+batchSize]\n",
    "            # GetBatch() returns the rendered images, the labelseq(GT) for each image and the lengths of each of the labelseq\n",
    "            images,labelSeqs,labelSeqlens =GetBatch(imWidth,batchOfWords,singleFont)\n",
    "            images=autograd.Variable(images)\n",
    "            # coniguous since we will be doing a view() of this later\n",
    "            images=images.contiguous()\n",
    "            labelSeqs=autograd.Variable(labelSeqs)\n",
    "            labelSeqlens=autograd.Variable(labelSeqlens)\n",
    "\n",
    "            if use_cuda:\n",
    "                images=images=images.cuda()\n",
    "            #do the forward pass\n",
    "            outputs=model(images)\n",
    "            outputs=outputs.contiguous()\n",
    "            #the size of the output activations, this is required when you call the CTC loss\n",
    "            outputsSize=autograd.Variable(torch.IntTensor([outputs.size(0)] * batchSize))\n",
    "            trainCost = criterion(outputs, labelSeqs, outputsSize, labelSeqlens) / batchSize\n",
    "\n",
    "            avgTrainCost+=trainCost\n",
    "            if i%10000==0:\n",
    "                avgTrainCost=avgTrainCost/(10000/batchSize)\n",
    "                #print ('avgTraincost for last 5000 samples is',avgTrainCost)\n",
    "                avgTrainCost=0\n",
    "                # forward the network with the validation iamges as input\n",
    "                valOutputs=model(valImages)\n",
    "                #print (valOutputs.size()) 100 X nvalsamoles x 37\n",
    "                valOutputs=valOutputs.contiguous()\n",
    "                valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "                valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "                print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "                # valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "                valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "                # second output of max() is the argmax along the requuired dimension\n",
    "                _, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "                #the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "                predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "                predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "                #print the predicted raw string and the decoded string for the valimages\n",
    "                for ii in range(0,5):\n",
    "\n",
    "                    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n",
    "                    #print(predictedStrings[ii])\n",
    "\n",
    "                    #   print (predictedSeqLabels[0,:].transpose(0,0))\n",
    "                #print(valOutputs_batchFirst[0,0,:])\n",
    "                #print (argMaxActivations[0,:])\n",
    "                print('Time since we began trainiing [%s]' % (time_since(start)))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            trainCost.backward()\n",
    "            optimizer.step()\n",
    "        print( 'completed  iteration no -', iter)\n",
    "        #if (iter%2==0 or valCost.data[0]  < 6   ) and saveTrue :\n",
    "        #    iterString=str(iter)\n",
    "        #    torch.save(model, 'ocrmodel_iter_'+iterString+'.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 44.895072937\n",
      "0z~~~~~~~~0z~0~==>0z0z0\n",
      "0z~~~~~~~~~~~0~==>0z0\n",
      "0z~~~~~~~~~~~0~==>0z0\n",
      "0z~~~~~~~~~~~~0==>0z0\n",
      "0z~~~~~~~~~~~~0==>0z0\n",
      "Time since we began trainiing [0m 0s]\n",
      "completed  iteration no - 0\n",
      "validaton Cost is 10.0740613937\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 1s]\n",
      "completed  iteration no - 1\n",
      "validaton Cost is 4.81364917755\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 1s]\n",
      "completed  iteration no - 2\n",
      "validaton Cost is 4.87295484543\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 2s]\n",
      "completed  iteration no - 3\n",
      "validaton Cost is 4.68374586105\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 3s]\n",
      "completed  iteration no - 4\n",
      "validaton Cost is 4.75127029419\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 3s]\n",
      "completed  iteration no - 5\n",
      "validaton Cost is 4.65097332001\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 3s]\n",
      "completed  iteration no - 6\n",
      "validaton Cost is 4.71274614334\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 4s]\n",
      "completed  iteration no - 7\n",
      "validaton Cost is 4.70609331131\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 4s]\n",
      "completed  iteration no - 8\n",
      "validaton Cost is 4.70190525055\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 5s]\n",
      "completed  iteration no - 9\n",
      "validaton Cost is 4.59642505646\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 5s]\n",
      "completed  iteration no - 10\n",
      "validaton Cost is 4.66007709503\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 6s]\n",
      "completed  iteration no - 11\n",
      "validaton Cost is 4.60775089264\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 7s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-18bcb86f1b97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mvalImages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrainNtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqlens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f1d0fff25fd5>\u001b[0m in \u001b[0;36mtrainNtest\u001b[1;34m(imWidth, valImages, valLabelSeqs, valLabelSeqlens, singleFont, saveTrue)\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m#do the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#the size of the output activations, this is required when you call the CTC loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ca7fb67f9edf>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#print ('size of x=', x.size()) # BxWxH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlstmOut1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblstm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m#x has three dimensions batchSize* seqLen * FeatDim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlstmOut1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mdropout_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         )\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcy_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             ))\n\u001b[0;32m    298\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### lets first try to overfit the model to some dummy data ###\n",
    "# we will use only words containing say only three characters a, b and c\n",
    "#and validation also will be words having only those chars\n",
    "\n",
    "# read a file with lots of 'words' comprising of just a b and c\n",
    "imWidth=15\n",
    "vocabFile=codecs.open('../../../data/lab2/small_lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "\n",
    "## validation data ##\n",
    "valWords=['cab','bbc','acc','bcc','bac']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(imWidth,valWords,1)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "\n",
    "    \n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "    \n",
    "trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,1,0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### now lets try on a larger data set, which is rendered from a large vocabulary of 90k words ##\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Prepare the synthetic validation data for the training\n",
    "##############\n",
    "imWidth=100\n",
    "valWords=['944678567','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(imWidth,valWords,1)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "\n",
    "    \n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 312.824584961\n",
      "a~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~x6==>ax6\n",
      "a~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~s==>as\n",
      "a~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~x~~==>ax\n",
      "a~~~~~~~~~~~~~~~~~~~~~~~~~~xa~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~xa~~~~~~~~~~~x6==>axaxax6\n",
      "a~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~x~~~==>ax\n",
      "Time since we began trainiing [0m 0s]\n",
      "validaton Cost is 37.8214149475\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 18s]\n",
      "validaton Cost is 39.8274307251\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 39s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e5059e7951aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainNtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqlens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# set the second last argument in above function call to 1 if it trains more than 10 minutes to converge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# if the second last argument is 1 then only one font will be used in rendering images and so it ll converge fast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#the last argument =1 means the models will be saved at regular intervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f1d0fff25fd5>\u001b[0m in \u001b[0;36mtrainNtest\u001b[1;34m(imWidth, valImages, valLabelSeqs, valLabelSeqlens, singleFont, saveTrue)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mbatchOfWords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# GetBatch() returns the rendered images, the labelseq(GT) for each image and the lengths of each of the labelseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelSeqlens\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mGetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchOfWords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msingleFont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# coniguous since we will be doing a view() of this later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-7264aad235d9>\u001b[0m in \u001b[0;36mGetBatch\u001b[1;34m(imWidth, batchOfWords, singleFont)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwordText\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimageFont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimHeight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#img.save(text+'.jpeg')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/PIL/ImageDraw.pyc\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self, xy, text, fill, font, anchor, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mink\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                 \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmask2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m                 \u001b[0mxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/PIL/ImageFont.pyc\u001b[0m in \u001b[0;36mgetmask2\u001b[1;34m(self, text, mode, fill)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,0,1)\n",
    "# set the second last argument in above function call to 1 if it trains more than 10 minutes to converge\n",
    "# if the second last argument is 1 then only one font will be used in rendering images and so it ll converge fast\n",
    "#the last argument =1 means the models will be saved at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained model and testing the validation data on it ###\n",
    "In case your networks takes lot of time to converge, we have a pretrained model for you. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 6.93511724472\n",
      "u~~~~~~~~~~~h~~~~~~~~~~4~~~~~~~~~~b~~~~~~~~~~~~~~~~~~~~~~d~~~~~~~~~~~~~~~~~~~~b~~~~~~~~~~~~~~~~~~~~y==>uh4bdby\n",
      "h~~~~~~~~~~~~~~~~~y~~~~d~~~~~~~~~~~e~~~~~~~~~~r~~~~~~~~a~~~~~~~~~b~~~~~~~~~~~~a~~~~~~~~~d~~~~~~~~~~~==>hyderabad\n",
      "g~~~~~~~~~~~~~o~~~~~~~~~~~~~l~~~~~c~~~~~~~~~~~o~~~~~~~~~~~~n~~~~~~~~~~~~~d~~~~~~~~~~~~~~a~~~~~~~~~~~==>golconda\n",
      "c~~~~~~~~~h~~~~~~~~~~~~a~~~~~~~~~r~~~~~~~~m~~~~~~~~~~~~~~~~~~~i~~~~n~~~~~~~~~~~~a~~~~~~~~~~r~~~~~~~~==>charminar\n",
      "g~~~~~~~~~~~a~~~~~~~~c~~~~~~~~~h~~~~~~~~~~~~i~~~~b~~~~~~~~~~o~~~~~~~~~~~w~~~~~~~~~~~~~~~l~~~~~~i~~~~==>gachibowli\n"
     ]
    }
   ],
   "source": [
    "#  load a saved model and test our test/validation data on it #\n",
    "\n",
    "model = torch.load(\"../../../data/lab2/ocr_valE5_blstm.pt\")\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "#model.load_state_dict(torch.load(\"../../../data/lab2/ocrmodel_iter_40.pt\n",
    "\n",
    "valOutputs=model(valImages)\n",
    "valOutputs=valOutputs.contiguous()\n",
    "valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "# valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "# second output of max() is the argmax along the requuired dimension\n",
    "_, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "#the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "#print the predicted raw string and the decoded string for the valimages\n",
    "for ii in range(0,5):\n",
    "\n",
    "    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CRNN ##\n",
    "- Now we will add a small convolutional stack at the networks head \n",
    "- The convolutional stack would act as a feature extractor \n",
    "- The convoultional stack added here is pretty similar to the one [here](http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) \n",
    "    - We have just two convolutional layers followed by a max pooling operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class crnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(crnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        # bidirectional= true to make the rnn bidirectional\n",
    "        #cnn stack\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        \n",
    "        \n",
    "        # rnn part\n",
    "        # lstm input size would be 64*6=384 always (numfilters after conv2=64 and the imageheight is 6 after the conv. stack)\n",
    "        self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "                \n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "                \n",
    "    def forward(self, x ):\n",
    "        \n",
    "        #x is BxHxW we maake IT BxCxHxW\n",
    "        x=x.unsqueeze(1) # we add an extra dimension at 1 for #channels\n",
    "        #see the input dimension required for conv2s\n",
    "        #print(x.size())\n",
    "        B,C,T,D=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #print('size of x in the beginning =', x.size()) # batxhSizexnumChannels=1xHxW\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # batchSizex64xH/2-1(W/2-1)x\n",
    "        #print('size of x after conv1 and pooling =', x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # batchSizex64xH/2/2-1x(W/2-1)/2-1\n",
    "        #print('size of x after conv2 and pooling =', x.size())\n",
    "        #if input is 50x1x32x100 then it would become 50x64x16*49 and then 50x64x6x23\n",
    "        #print(x.size())\n",
    "        x=x.contiguous()\n",
    "        B,C,D,T=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #x=x.transpose(2,3) #swapping last two dimensions\n",
    "        x=x.contiguous()\n",
    "        x=x.view(B,x.size(1)*x.size(2),-1) # BxC*DXT\n",
    "        x=x.transpose(1,2) #making T the second dimension\n",
    "        #print(x.size())\n",
    "        \n",
    "        \n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "                \n",
    "        # output of RNN is reshaped to B*T x D before it is fed to the linear layer\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        # the activations are reshaped to B x T x outputDim size\n",
    "        #then a transpose of B and T since CTC expects the T to be first\n",
    "        outputLayerActivations= outputLayerActivations.view(B,T,-1).transpose(0,1)\n",
    "        #if use_cuda:\n",
    "        #    outputLayerActivations=outputLayerActivations.cuda()\n",
    "        return outputLayerActivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = crnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "if use_cuda:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 63.7241134644\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "pc~~~~~~~~~~~~~~~~~~~~~==>pc\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "Time since we began trainiing [0m 0s]\n",
      "validaton Cost is 63.7241134644\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "pc~~~~~~~~~~~~~~~~~~~~~==>pc\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "c~~~~~~~~~~~~~~~~~~~~~~==>c\n",
      "Time since we began trainiing [0m 17s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c524879b1cdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtrainNtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqlens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-42cfd3a1d172>\u001b[0m in \u001b[0;36mtrainNtest\u001b[1;34m(imWidth, valImages, valLabelSeqs, valLabelSeqlens, singleFont, saveTrue)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mbatchOfWords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# GetBatch() returns the rendered images, the labelseq(GT) for each image and the lengths of each of the labelseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelSeqlens\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mGetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchOfWords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msingleFont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# coniguous since we will be doing a view() of this later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-7264aad235d9>\u001b[0m in \u001b[0;36mGetBatch\u001b[1;34m(imWidth, batchOfWords, singleFont)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#img.save(text+'.jpeg')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mimgTensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage2tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mimgTensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimgTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# at 0 a new dimenion is added\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-40b57dc2c077>\u001b[0m in \u001b[0;36mimage2tensor\u001b[1;34m(im)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mgreyscale_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mgreyscale_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreyscale_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mgreyscale_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgreyscale_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mgreyscale_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreyscale_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## loading the vocabulary file once again\n",
    "#to make sure that its not the smaller vocabfile in use now\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "\n",
    "## the validation data\n",
    "###########\n",
    "# Prepare the synthetic validation data for the training\n",
    "##############\n",
    "imWidth=100\n",
    "valWords=['944678567','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(imWidth,valWords,1)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "\n",
    "    \n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "\n",
    "    \n",
    "###\n",
    "trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
