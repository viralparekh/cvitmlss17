{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Training an OCR using CNN + RNN + CTC on Synthetic Images ##\n",
    "- Now we change the network architecture slightly ; we add a convolutional stack before the BLSTM layer\n",
    "    - Now your input to the network is not the raw pixel values, But we do steps of convolution and maxpooling and the resultant output is reshpaed to form a Time x featDim structured before it is fed to the network. \n",
    "    - The convoultional stack can be increased in depth to get better feature represenations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Upto the model definition part, the code is exactly same as Q3-A . You  may skip reading it but dont forget to run all of them</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Use a BRNN + CTC to recognize given word image \n",
    "# Network is trained on images rendered using PIL \n",
    "# ============================================================================\n",
    "# \n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import numpy as np\n",
    "import time,math\n",
    "from time import sleep\n",
    "import random\n",
    "import sys,codecs,glob \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "random.seed(0)\n",
    "# TODO - MAKE SURE CTC IS INSTALLED IN ALL MACHINES\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary and the fonts ####\n",
    "-  loading the lexicon of 90k words\n",
    "- get the fontslist to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all word images are resized to a height of 32 pixels\n",
    "imHeight=32 \n",
    "\"\"\"\n",
    "image width is also set a fixed size\n",
    "YES. Though RNNS can handle variable length sequences we resize them to fixed width\n",
    "This is for the ease of batch learning\n",
    "And it doesnt seem to affect the performance much atleast in our case\n",
    "\n",
    "Pytorch provides a packed array API incase you want to have variable length sequences within a batch\n",
    "see the discussion here\n",
    "https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120/8\n",
    "\n",
    "\"\"\"\n",
    "#imWidth=100\n",
    "imWidth=100\n",
    "fontsList=glob.glob('../../../data/lab2/googleFonts/'+'*.ttf')\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "fontSizeOptions={'16','20','24','28','30','32','36','38'}\n",
    "batchSize=5\n",
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "#alphabet=\"(3)-\"\n",
    "dict={}\n",
    "for i, char in enumerate(alphabet):\n",
    "\tdict[char] = i + 1\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple helper function to compute time since some 'start time'\n",
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return the class labels for each character in the targetsequence \n",
    "def Str2Labels(text):\n",
    "\tglobal dict\n",
    "\ttext = [dict[char.lower()] for char in text]\n",
    "\t#print (text)\n",
    "\tlength=len(text)\n",
    "\treturn text, length\n",
    "#StrtoLabels(\"0-1\")\n",
    "\n",
    "### from the predicted sequence of labels for an image, decode the string\n",
    "# function returns the rawstring and also the decoded string after removing blanks and duplicates\n",
    "\n",
    "#eg: if labelsequnce you get after an argmax on the output activation matris is  [12,12,0,0,15,0,15,15,0,0]\n",
    "#then your raw label string would be \"bb~~e~ee~~\" and the outputstring \"bee\"\n",
    "def Labels2Str(predictedLabelSequences):\n",
    "    bz=predictedLabelSequences.size(0)\n",
    "    predictedRawStrings=[]\n",
    "    predictedStrings=[]\n",
    "    for i in range(0,bz):\n",
    "        predictedRawString=\"\"\n",
    "        predictedString=\"\"\n",
    "        predictedLabelSeq=predictedLabelSequences.data[i,:]\n",
    "        prevId=1000 #just a large value which is not in the index \n",
    "        character=\"\"\n",
    "        character_raw=\"\"\n",
    "        for j in range (0, predictedLabelSeq.size(0)):\n",
    "            idx=predictedLabelSeq[j]\n",
    "            if (prevId != 1000 or prevId!=idx) :\n",
    "                if prevId!=idx:\n",
    "                    if idx==0:\n",
    "                        character_raw=\"~\"\n",
    "                        character=\"\"\n",
    "                    else:\n",
    "                        character_raw=alphabet[idx-1]\n",
    "                        character=alphabet[idx-1]\n",
    "                else:\n",
    "                    character_raw=\"~\"\n",
    "                    character=\"\"\n",
    "                prevId=idx\n",
    "            else:\n",
    "                character=\"\"\n",
    "                if idx==0:\n",
    "                    character_raw=\"~\"\n",
    "                else:\n",
    "                    character_raw=alphabet[idx-1]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            predictedString+=character\n",
    "            predictedRawString+=character_raw\n",
    "        predictedRawStrings.append(predictedRawString)\n",
    "        predictedStrings.append(predictedString)\n",
    "        \n",
    "    return predictedRawStrings, predictedStrings\n",
    "\n",
    "\n",
    "\n",
    "def image2tensor(im):\n",
    "    #returns the pixel values of a PIL image (in 0-1 range) as a numpy 2D array\n",
    "\n",
    "    (width, height) = im.size\n",
    "    greyscale_map = list(im.getdata())\n",
    "    greyscale_map = np.array(greyscale_map, dtype = np.uint8)\n",
    "    greyscale_map=greyscale_map.astype(float)\n",
    "    greyscale_map = torch.from_numpy(greyscale_map.reshape((height, width))).float()/255.0\n",
    "    return greyscale_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images, prepare a training batch ###\n",
    "- renders a batch of word images, from the list of words supplied\n",
    "- if singleFont is true then only one font would be used to render images. This is useful in case where you want to test overfitting the network to easy examples\n",
    "- Along with the rendered images, the target strings are converted to corresponding sequence of labels; for example the word \"bee\" would be converted to [12,15,15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetBatch ( batchOfWords ):\n",
    "\t\"\"\"\n",
    "\tRenders a batch of word images and returns the images along with the corresponding GTs\n",
    "\tUses PIL to render word images\n",
    "\tfont is randomly picked from a set of freely available google fonts\n",
    "\tword is picked from a vocabulary of English words\n",
    "\n",
    "\t\"\"\"\n",
    "\twordImages=[]\n",
    "\tlabelSequences=[]\n",
    "\tlabelSeqLengths=[]\n",
    "\n",
    "\tfor  i,text in enumerate (batchOfWords):\n",
    "\t\twordText=text\n",
    "\t\t#print('text is', text)\n",
    "\t\tfontName=fontsList[0]\n",
    "\t\tfontSize='26'\n",
    "\t\t#fontSize=fontSizeOptions[0]\n",
    "\t\tfontName=random.sample(fontsList,1)[0]\n",
    "\t\tfontSize=random.sample(fontSizeOptions,1)[0]\n",
    "\t\timageFont = ImageFont.truetype(fontName,int(fontSize))\n",
    "\t\ttextSize=imageFont.getsize(wordText)\n",
    "\t\timg=Image.new(\"L\", textSize,(255))\n",
    "\t\tdraw = ImageDraw.Draw(img)\n",
    "\t\tdraw.text((0, 0),wordText,(0),font=imageFont)\n",
    "\t\timg=img.resize((imWidth,imHeight), Image.ANTIALIAS)\n",
    "\t\t#img.save(text+'.jpeg')\n",
    "\n",
    "\t\timgTensor=image2tensor(img)\n",
    "\t\timgTensor=imgTensor.unsqueeze(0) # at 0 a new dimenion is added\n",
    "\n",
    "\t\twordImages.append(imgTensor)\n",
    "\n",
    "\t\tlabelSeq,l=Str2Labels(wordText)\n",
    "\t\tlabelSequences+=labelSeq\n",
    "\t\tlabelSeqLengths.append(l)\n",
    "\tbatchImageTensor=torch.cat(wordImages,0) #now all the image tensors are combined ( we  did the unsqueeze eariler for this cat)\t\n",
    "\t#batchImageTensor=torch.transpose(batchImageTensor,1,2)\n",
    "\tlabelSequencesTensor=torch.IntTensor(labelSequences)\n",
    "\tlabelSeqLengthsTensor=torch.IntTensor(labelSeqLengths)\n",
    "\treturn batchImageTensor, labelSequencesTensor, labelSeqLengthsTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a convolutional stack to the BLSTM + CTC Architecure ###\n",
    "Remember that earlier we were feeding raw pixel columns as inputs at each time step <br>\n",
    "Here we will use a covolutional stack to get some useful represenations from the word image <br>\n",
    "And then sequences of this convolutional features are fed to the BLSTM layer above <br>\n",
    "\n",
    "![CRNN Architecture](crnnstack.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minesh TODO split blstm into a separate class ?\n",
    "\n",
    "class crnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(crnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        # bidirectional= true to make the rnn bidirectional\n",
    "        #cnn stack\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        \n",
    "        \n",
    "        # rnn part\n",
    "        if numDirections==2:\n",
    "            self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "        else:\n",
    "            self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=False, batch_first=True)\n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "                \n",
    "    def forward(self, x ):\n",
    "        # x will be in shape B x D x T\n",
    "        x=x.unsqueeze(1) # # we add an extra dimension at 1 for #channels\n",
    "        #see the input dimension required for conv2s\n",
    "        #print(x.size())\n",
    "        \n",
    "        #print('size of x in the beginning =', x.size()) # batxhSizexnumChannels=1xHxW\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # batchSizex64xH/2-1(W/2-1)x\n",
    "        #print('size of x after conv1 and pooling =', x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # batchSizex64xH/2/2-1x(W/2-1)/2-1\n",
    "        #print('size of x after conv2 and pooling =', x.size())\n",
    "        #if input is 50x1x32x100 then it would become 50x64x16*49 and then 50x64x6x23\n",
    "        #print(x.size())\n",
    "        x=x.contiguous()\n",
    "        B,C,D,T=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #x=x.transpose(2,3) #swapping last two dimensions\n",
    "        x=x.contiguous()\n",
    "        x=x.view(B,x.size(1)*x.size(2),-1) # BxC*DXT\n",
    "        x=x.transpose(1,2) #making T the second dimension\n",
    "        #print(x.size())\n",
    "        \n",
    "        \n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "                \n",
    "        # output of RNN is reshaped to B*T x D before it is fed to the linear layer\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        # the activations are reshaped to B x T x outputDim size\n",
    "        #then a transpose of B and T since CTC expects the T to be first\n",
    "        outputLayerActivations= outputLayerActivations.view(B,T,-1).transpose(0,1)\n",
    "        #if use_cuda:\n",
    "        #    outputLayerActivations=outputLayerActivations.cuda()\n",
    "        return outputLayerActivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1  ###\n",
    "1. During the above convolution and max pooling operations, what is the stride size used ?\n",
    "2. Why is first argument to nn.LSTM() is 384 ? What was this value earlier when we didnt have a convolutional stack?\n",
    "3. Would the value 384 change if If i change the WIdth of the input image ? Give reasons for your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Prepare the synthetic validation data\n",
    "##############\n",
    "\n",
    "valWords=['9446567542','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(valWords)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To handle out of memory error ###\n",
    " - First try making the batchSize smaller\n",
    " - then you can make the network unidirectional one by setting numDirections=1\n",
    " - or change the the number of hidden units\n",
    " - in the worst case make imWidth smaller ( this is set at the beginning of the code)\n",
    " - if nothing above works uncomment the line use_cuda=False at the beggining \n",
    " - If you are unable to do the training even after trying out above, you should load a pretrained model and see the results (at the end of this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 63.6886291504\n",
      "j~5~~~~~~~~~~~~~~~~~~~~==>j5\n",
      "j5~~~~~~~~~~~~~~~~~~~~~==>j5\n",
      "j5~~~~~~~~~~~~~~~~~~~~~==>j5\n",
      "j~5~~~~~~~~~~~~~~~~~~~~==>j5\n",
      "j5~~~~~~~~~~~~~~~~~~~~~==>j5\n",
      "Time since we began trainiing [0m 0s]\n",
      "validaton Cost is 39.3794822693\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 1s]\n",
      "validaton Cost is 40.5147285461\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 3s]\n",
      "validaton Cost is 41.538898468\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 5s]\n",
      "validaton Cost is 42.2752532959\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 6s]\n",
      "validaton Cost is 42.7451133728\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 8s]\n",
      "validaton Cost is 43.046672821\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 9s]\n",
      "validaton Cost is 44.2651748657\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "Time since we began trainiing [0m 11s]\n",
      "validaton Cost is 43.8424568176\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "Time since we began trainiing [0m 12s]\n",
      "validaton Cost is 43.9397659302\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "Time since we began trainiing [0m 14s]\n",
      "validaton Cost is 44.2947921753\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 15s]\n",
      "validaton Cost is 45.2655105591\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "Time since we began trainiing [0m 17s]\n",
      "validaton Cost is 43.7813568115\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [0m 18s]\n",
      "validaton Cost is 40.9012374878\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "s~~~~~~~~~~~~~~~~~~~~~s==>ss\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "c~~~~~~~~~~~~~~~~~~~~~y==>cy\n",
      "Time since we began trainiing [0m 20s]\n",
      "validaton Cost is 41.3250427246\n",
      "b~~~~~~~~~~~~~~~~~~~~~~==>b\n",
      "b~~~~~~~~~~~~~~~~~~~~~s==>bs\n",
      "~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~d==>d\n",
      "Time since we began trainiing [0m 21s]\n",
      "validaton Cost is 40.5320014954\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "b~~~~~~~~~~~~~~~~~~~~~y==>by\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~n~==>n\n",
      "~~~~~~~~~~~~~~~~~~~~~~d==>d\n",
      "Time since we began trainiing [0m 22s]\n",
      "validaton Cost is 36.9837226868\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "b~~~~~~le~~~~e~~t~~e~~g==>bleeteg\n",
      "c~~~~~l~~~~~~~~~~~~~l~~==>cll\n",
      "c~~~~~~~~~~~~~~i~~~~~~r==>cir\n",
      "m~~~~~e~t~i~t~~~~~~~~l~==>metitl\n",
      "Time since we began trainiing [0m 24s]\n",
      "validaton Cost is 35.7800865173\n",
      "~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "t~~~~a~te~~~~e~~t~~e~~g==>tateeteg\n",
      "c~~~~~~ie~~~~~~~~~~~~~~==>cie\n",
      "c~~~~~~~~~~~~~~i~~~~~~r==>cir\n",
      "m~~~~~e~t~i~t~~~~~~~~~i==>metiti\n",
      "Time since we began trainiing [0m 25s]\n",
      "validaton Cost is 31.3293266296\n",
      "~~~~~t~o~~~~f~~~~~~~i~n==>tofin\n",
      "h~~~~g~~e~~r~a~~h~~e~~g==>hgeraheg\n",
      "g~~~~~l~~~~~~~~m~~~d~~~==>glmd\n",
      "c~h~~~o~~r~~~~~i~~~n~~r==>chorinr\n",
      "m~~m~~~~t~i~t~~o~~~~~l~==>mmtitol\n",
      "Time since we began trainiing [0m 27s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1ff9195758f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mlabelSeqlens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelSeqlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moutputsSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-d1c3c269e036>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mlstmOut1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblstm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m#x has three dimensions batchSize* seqLen * FeatDim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mlstmOut1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmOut1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mdropout_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         )\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0mhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# TRAINING\n",
    "##################################################\n",
    "\"\"\"\n",
    "a batch of words are sequentially fetched from the vocabulary\n",
    "one epoch runs until all the words in the vocabulary are seen once\n",
    "then the word list is shuffled and above process is repeated\n",
    "\"\"\"\n",
    "batchSize=5\n",
    "nHidden=80\n",
    "nClasses= len(alphabet)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "numLayers=2 # the 2 BLSTM layers defined seprately without using numLayers option for nn.LSTM\n",
    "numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "model = crnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "start = time.time()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "\n",
    "for iter in range (0,200):\n",
    "    avgTrainCost=0\n",
    "    random.shuffle(words)\n",
    "\n",
    "    for i in range (0,vocabSize-batchSize+1,batchSize):\n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        batchOfWords=words[i:i+batchSize]\n",
    "        images,labelSeqs,labelSeqlens =GetBatch(batchOfWords)\n",
    "        images=autograd.Variable(images)\n",
    "        #images=autograd.Variable(images)\n",
    "        images=images.contiguous()\n",
    "        if use_cuda:\n",
    "            images=images.cuda()\n",
    "        labelSeqs=autograd.Variable(labelSeqs)\n",
    "\n",
    "        labelSeqlens=autograd.Variable(labelSeqlens)\n",
    "        outputs=model(images)\n",
    "        outputs=outputs.contiguous()\n",
    "        outputsSize=autograd.Variable(torch.IntTensor([outputs.size(0)] * batchSize))\n",
    "        trainCost = criterion(outputs, labelSeqs, outputsSize, labelSeqlens) / batchSize\n",
    "\n",
    "        avgTrainCost+=trainCost\n",
    "        if i%500==0:\n",
    "            avgTrainCost=avgTrainCost/(5000/batchSize)\n",
    "            #print ('avgTraincost for last 5000 samples is',avgTrainCost)\n",
    "            avgTrainCost=0\n",
    "            valOutputs=model(valImages)\n",
    "#print (valOutputs.size()) 100 X nvalsamoles x 37\n",
    "            valOutputs=valOutputs.contiguous()\n",
    "            valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "            valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "            print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "            ### get the actual predictions and compute word error ################\n",
    "            valOutputs=valOutputs.transpose(0,1)\n",
    "            # second output of max() is the argmax along the requuired dimension\n",
    "            _, argMaxActivations= valOutputs.max(2)\n",
    "            #the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "            predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen\n",
    "            predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "            for ii in range(0,5):\n",
    "\n",
    "                print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n",
    "\n",
    "            #   print (predictedSeqLabels[0,:].transpose(0,0))\n",
    "            #print(valOutputs_batchFirst[0,0,:])\n",
    "            print('Time since we began trainiing [%s]' % (time_since(start)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        trainCost.backward()\n",
    "        optimizer.step()\n",
    "    print( 'completed  iteration no -', iter)\n",
    "    #if (iter%2==0 or valCost.data[0]  < 6   )  :\n",
    "    #    iterString=str(iter)\n",
    "    #    torch.save(model, 'ocrmodel_iter_'+iterString+'.pt')\n",
    "    #iterString=int(iter)\n",
    "    #torch.save(model.state_dict(), iterString+'.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained model and testing the validation data on it ###\n",
    "In case your networks takes lot of time to converge, we have a pretrained model for you. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 17.3930625916\n",
      "s~u~~a~a~~s~s~~i~a~~s~e==>suaassiase\n",
      "h~~y~d~~e~~r~a~~b~~a~~d==>hyderabad\n",
      "g~~o~~l~c~~o~~~n~~d~~a~==>golconda\n",
      "c~h~~a~~r~~n~li~n~~a~~r==>charnlinar\n",
      "g~~a~c~~h~i~~b~o~~u~l~i==>gachibouli\n"
     ]
    }
   ],
   "source": [
    "#  load a saved model and test our test/validation data on it #\n",
    "use_cuda=True #since the saved model is a gpu model\n",
    "model = torch.load(\"../../../data/lab2/crnnmodel.pt\")\n",
    "criterion = CTCLoss()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "\n",
    "valWords=['9446567456','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(valWords)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "valOutputs=model(valImages)\n",
    "valOutputs=valOutputs.contiguous()\n",
    "valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "# valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "# second output of max() is the argmax along the requuired dimension\n",
    "_, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "#the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "#print the predicted raw string and the decoded string for the valimages\n",
    "for ii in range(0,5):\n",
    "\n",
    "    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise ###\n",
    "0. Did you notice that length of the predictedRawstrings is shorter in this case? For example rawprediction of hyderabad is \"h~~y~d~~e~~r~a~~b~~a~~d\". Eariler the raw string was of length=100 (=imWidth). Why is the length shorter now?\n",
    "1. Modify the Labels2Str() function to return the word accuracy along with the other returned values\n",
    "2. Have you noticed that numbers are poorly recognized using both our architecures. Can you reason why the recognition accuracies are poorer for numbers\n",
    "    - hint -look at your vocabulary --  find out how many 'words' with numbers are in there\n",
    "3. In general when the image is a sequence of numbers, RNNs perform poorly compared to its performance on recognizing words in a language. What could be the reason why RNNs perform poorer on a sequence of numbers\n",
    "    - hint - Rnn's memory itself is the villain\n",
    "4. You were introduced to beam search as an an alternative to argMax decoding while solving seq2seq problems. Can you write a beam search decoding for our problem and see if the recognition results are different with beam search (You may use existing code. librararies for beam search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
