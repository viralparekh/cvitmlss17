{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training an OCR using RNN + CTC on Synthetic Images ##\n",
    "- To train neural network to do seq2seq mapping, when your input sequence and output sequence are not aligned\n",
    "- Input sequence is a sequence of image features and output is a sequence of characters\n",
    "- Images are resized to  fixed width , though we can have variying widths since RNN can handle variable length sequences. This helps in faster batch learning\n",
    "- Training images are rendered on the fly for the task\n",
    "- The network is tested on synthetic images, but rendered from out-of-vocabulary words\n",
    "- We train a network with a bidirectional RNN  and a CTC loss for the task\n",
    "     - A word image's each column is treated as a timestep. so inputdim= height of the word image and seqlen= width of the image\n",
    "     - Here to make sure the networks learns the mappings we first overfit it to 3 letter words\n",
    "     - Then we will the train network on a larger dataset, comprising of images rendered from 90k English words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Use a BRNN + CTC to recognize given word image \n",
    "# Network is trained on images rendered using PIL \n",
    "# ============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "from __future__ import print_function\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys,codecs,glob \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import time,math\n",
    "from time import sleep\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "random.seed(0)\n",
    "# TODO - MAKE SURE CTC IS INSTALLED IN ALL MACHINES\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "use_cuda=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary and the fonts ####\n",
    "-  loading the lexicon of 90k words\n",
    "- get the fontslist to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all word images are resized to a height of 32 pixels\n",
    "imHeight=32 \n",
    "\"\"\"\n",
    "image width is also set a fixed size\n",
    "YES. Though RNNS can handle variable length sequences we resize them to fixed width\n",
    "This is for the ease of batch learning\n",
    "\n",
    "\"\"\"\n",
    "#imWidth=100\n",
    "imWidth=100\n",
    "#65 google fonts are used\n",
    "fontsList=glob.glob('../../../data/lab2/googleFonts/'+'*.ttf')\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "#90k vocabulary\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "fontSizeOptions={'16','20','24','28','30','32','36','38'}\n",
    "batchSize=5 \n",
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "#alphabet=\"(3)-\"\n",
    "dict={}\n",
    "for i, char in enumerate(alphabet):\n",
    "\tdict[char] = i + 1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple helper function to compute time since some 'start time'\n",
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting from strings to Labels ###\n",
    "- Your GT for each image is word. We will convert the word into sequences of labels.<br>\n",
    "- Each character in your alphabet is mapped to a number <r>\n",
    "- And so your GT becomes a sequence of labels <br>\n",
    "- Note that the class labels begin from 1 not 0<br>\n",
    "- 0 is rserved for blank label in CTC\n",
    "\n",
    "### converting from Labels Strings - argMax Decoding ###\n",
    "- Network would give you an activation vector - probability distribution over the output class labels. At each time step we find out the class/label for which the probability is the max.\n",
    "- We find out the classes with highest probability at all time steps (this is called argMax decoding) \n",
    "- So after decoding you will have a label for each time step ( This will be your predictedRawString)\n",
    "- Now we will remove the recurring characters and blank labels from the above sequence of labels you will get the final predicted string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the class labels for each character in the targetsequence \n",
    "def Str2Labels(text):\n",
    "\tglobal dict\n",
    "\ttext = [dict[char.lower()] for char in text]\n",
    "\t#print (text)\n",
    "\tlength=len(text)\n",
    "\treturn text, length\n",
    "#StrtoLabels(\"0-1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### from the predicted sequence of labels for an image, decode the string\n",
    "# function returns the rawstring and also the decoded string after removing blanks and duplicates\n",
    "\n",
    "#eg: if labelsequnce you get after an argmax on the output activation matris is  [12,12,0,0,15,0,15,15,0,0]\n",
    "#then your raw label string would be \"bb~~e~ee~~\" and the outputstring \"bee\"\n",
    "def Labels2Str(predictedLabelSequences):\n",
    "    bz=predictedLabelSequences.size(0)\n",
    "    predictedRawStrings=[]\n",
    "    predictedStrings=[]\n",
    "    for i in range(0,bz):\n",
    "        predictedRawString=\"\"\n",
    "        predictedString=\"\"\n",
    "        predictedLabelSeq=predictedLabelSequences.data[i,:]\n",
    "        prevId=1000 #just a large value which is not in the index \n",
    "        character=\"\"\n",
    "        character_raw=\"\"\n",
    "        for j in range (0, predictedLabelSeq.size(0)):\n",
    "            idx=predictedLabelSeq[j]\n",
    "            if (prevId != 1000 or prevId!=idx) :\n",
    "                if prevId!=idx:\n",
    "                    if idx==0:\n",
    "                        character_raw=\"~\"\n",
    "                        character=\"\"\n",
    "                    else:\n",
    "                        character_raw=alphabet[idx-1]\n",
    "                        character=alphabet[idx-1]\n",
    "                else:\n",
    "                    character_raw=\"~\"\n",
    "                    character=\"\"\n",
    "                prevId=idx\n",
    "            else:\n",
    "                character=\"\"\n",
    "                if idx==0:\n",
    "                    character_raw=\"~\"\n",
    "                else:\n",
    "                    character_raw=alphabet[idx-1]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            predictedString+=character\n",
    "            predictedRawString+=character_raw\n",
    "        predictedRawStrings.append(predictedRawString)\n",
    "        predictedStrings.append(predictedString)\n",
    "        \n",
    "    return predictedRawStrings, predictedStrings\n",
    "\n",
    "\n",
    "\n",
    "def image2tensor(im):\n",
    "    #returns the pixel values of a PIL image (in 0-1 range) as a numpy 2D array\n",
    "\n",
    "    (width, height) = im.size\n",
    "    greyscale_map = list(im.getdata())\n",
    "    greyscale_map = np.array(greyscale_map, dtype = np.uint8)\n",
    "    greyscale_map=greyscale_map.astype(float)\n",
    "    greyscale_map = torch.from_numpy(greyscale_map.reshape((height, width))).float()/255.0\n",
    "    return greyscale_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images, prepare a training batch ###\n",
    "- renders a batch of word images, from the list of words supplied\n",
    "- if singleFont is true then only one font would be used to render images. This is useful in case where you want to test overfitting the network to easy examples\n",
    "- Along with the rendered images, the target strings are converted to corresponding sequence of labels; for example the word \"bee\" would be converted to [12,15,15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetBatch ( batchOfWords ):\n",
    "\t\"\"\"\n",
    "\tRenders a batch of word images and returns the images along with the corresponding GTs\n",
    "\tUses PIL to render word images\n",
    "\tfont is randomly picked from a set of freely available google fonts\n",
    "\tword is picked from a vocabulary of English words\n",
    "\n",
    "\t\"\"\"\n",
    "\twordImages=[]\n",
    "\tlabelSequences=[]\n",
    "\tlabelSeqLengths=[]\n",
    "\n",
    "\tfor  i,text in enumerate (batchOfWords):\n",
    "\t\twordText=text\n",
    "\t\t#print('text is', text)\n",
    "\t\tfontName=fontsList[0]\n",
    "\t\tfontSize='26'\n",
    "\t\t#fontSize=fontSizeOptions[0]\n",
    "\t\tfontName=random.sample(fontsList,1)[0]\n",
    "\t\tfontSize=random.sample(fontSizeOptions,1)[0]\n",
    "\t\timageFont = ImageFont.truetype(fontName,int(fontSize))\n",
    "\t\ttextSize=imageFont.getsize(wordText)\n",
    "\t\timg=Image.new(\"L\", textSize,(255))\n",
    "\t\tdraw = ImageDraw.Draw(img)\n",
    "\t\tdraw.text((0, 0),wordText,(0),font=imageFont)\n",
    "\t\timg=img.resize((imWidth,imHeight), Image.ANTIALIAS)\n",
    "\t\t#img.save(text+'.jpeg')\n",
    "\n",
    "\t\timgTensor=image2tensor(img)\n",
    "\t\timgTensor=imgTensor.unsqueeze(0) # at 0 a new dimenion is added\n",
    "\n",
    "\t\twordImages.append(imgTensor)\n",
    "\n",
    "\t\tlabelSeq,l=Str2Labels(wordText)\n",
    "\t\tlabelSequences+=labelSeq\n",
    "\t\tlabelSeqLengths.append(l)\n",
    "\tbatchImageTensor=torch.cat(wordImages,0) #now all the image tensors are combined ( we  did the unsqueeze eariler for this cat)\t\n",
    "\tbatchImageTensor=torch.transpose(batchImageTensor,1,2)# BxHxW -> BxWxH for it to be in BxTxD shape\n",
    "\tlabelSequencesTensor=torch.IntTensor(labelSequences)\n",
    "\tlabelSeqLengthsTensor=torch.IntTensor(labelSeqLengths)\n",
    "\treturn batchImageTensor, labelSequencesTensor, labelSeqLengthsTensor\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ###\n",
    "1. For the words 'me', 'you' and 'i' call the function GetBatch(), and display the rendered images along with their labelsequences\n",
    "    - note that your input to GetBatch() is a list of words ; \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defintion  ###\n",
    "![OCR Architecture](blstm.jpg)\n",
    "- Input image here is of shape 100*32. Hence seqLen=100 and your featDim at a timestep =32\n",
    "- The below network has two BLSTM layers with #neurons in each layer = hiddenDim\n",
    "- the outputs of both the forward and backward recurrent layers in the second hidden layer are connected to a linear layer. There are hiddenDim*2 connections coming to this layer and its output is of size=outputDim=nClasses+1 (one extra class for blank label of CTC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class rnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(rnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        if numDirections==2:\n",
    "\n",
    "            self.blstm1=nn.LSTM(inputDim, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "        else:\n",
    "            self.blstm1=nn.LSTM(inputDim, hiddenDim,numLayers, bidirectional=False, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "        \n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x ):\n",
    "        B,T,D  = x.size(0), x.size(1), x.size(2)\n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "        \n",
    "\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        return outputLayerActivations.view(B,T,-1).transpose(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 ###\n",
    "1. In the above model definition, why are there  <i> hiddenDim*numDirections</i> connections to the output layer ?\n",
    "    - write your answer here itself in one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Prepare the synthetic validation data\n",
    "##############\n",
    "\n",
    "valWords=['9446567456','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(valWords)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To handle out of memory error ###\n",
    " - First try making the batchSize smaller\n",
    " - then you can make the network unidirectional one by setting numDirections=1\n",
    " - or change the the number of hidden units\n",
    " - in the worst case make imWidth smaller ( this is set at the beginning of the code)\n",
    " - if nothing above works uncomment the line use_cuda=False at the beggining \n",
    " - If you are unable to do the training even after trying out above, you should load a pretrained model and see the results (at the end of this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 ###\n",
    "1. Why does reducing the width of the image helps us in saving memory?\n",
    "    \n",
    "    \n",
    "2. It was mentioned earlier that you can actually pass variable length sequences to the RNN. Which means we may use variying width images to train our network. But can we have images with variying height fed to our network? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC loss ###\n",
    "- [This](https://github.com/SeanNaren/warp-ctc/tree/pytorch_bindings/pytorch_binding) to understand the arguments for CTC loss\n",
    "- [Original warp_ctc implemenation](https://github.com/baidu-research/warp-ctc)\n",
    "\n",
    "\n",
    "CTC loss function takes 4 arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 317.145751953\n",
      "y~~~~~~~~~~f~4y~f~y~f~~4~yf~y~f~~~y~~~~~~~~~~~~~~~~f~~y~~~~~~~~f~~y~~~f~~y~f~~yf~~y~~~~~~~f~~y~~~~4~==>yf4yfyf4yfyfyfyfyfyfyfyfy4\n",
      "y~~~~~~~~~~~~~~~~~~~~~~4y~~~~~~~~~~~~~~~~~~~~fy~~~~~4y~~~~~~~~~~~~~~~~~~4~y~~~~~~~~~~~~4~y~~~~~~4~~~==>y4yfy4y4y4y4\n",
      "y~~~~~~~~~~4~~y~~~~~~~f4~~y~~~4~~~y~~~~~~4~~y~~~~~~~~~4y~~~~~~~~~~~~4~~~y~~~~~~~~~f4~~~~~~~~~~y~~4~~==>y4yf4y4y4y4y4yf4y4\n",
      "yf4~~~~y~~4~y~~~~~4y~~~~4y~~~4~y~4~~~~~~~~~~~~~~~~~~~~~~~~~y4~~~~y~~4~~~~~~~~~y~~~4y~~~4~y~4~~~~~~~~==>yf4y4y4y4y4y4y4y4y4y4y4\n",
      "y~4f~~~~y~~~f~~~~4~~~f~4~~~~~~~f~~4y~4~f4y~f~~~yf~~~4~~~~~~~~~~~~~~~~~~~~~~~~f4~~~~f4~~~f~~y~~f~~4~~==>y4fyf4f4f4y4f4yfyf4f4f4fyf4\n",
      "Time since we began trainiing [0m 0s]\n",
      "validaton Cost is 36.8741111755\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [1m 7s]\n",
      "validaton Cost is 38.2477493286\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [2m 17s]\n",
      "validaton Cost is 39.509677887\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [3m 23s]\n",
      "validaton Cost is 40.4750976562\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [4m 33s]\n",
      "validaton Cost is 40.865322113\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [5m 41s]\n",
      "validaton Cost is 41.1385688782\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [6m 42s]\n",
      "validaton Cost is 42.0900535583\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [7m 44s]\n",
      "validaton Cost is 42.5039978027\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [8m 50s]\n",
      "validaton Cost is 42.2102661133\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [9m 58s]\n",
      "validaton Cost is 41.9504547119\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [11m 23s]\n",
      "validaton Cost is 43.2239952087\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~s==>s\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [12m 24s]\n",
      "validaton Cost is 42.699306488\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [13m 23s]\n",
      "validaton Cost is 43.4678497314\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [14m 29s]\n",
      "validaton Cost is 43.8982467651\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>\n",
      "Time since we began trainiing [15m 36s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e252b402f13b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mtrainCost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m#iterString=int(iter)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[0;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# TRAINING\n",
    "##################################################\n",
    "\"\"\"\n",
    "a batch of words are sequentially fetched from the vocabulary\n",
    "one epoch runs until all the words in the vocabulary are seen once\n",
    "then the word list is shuffled and above process is repeated\n",
    "\"\"\"\n",
    "nHidden=80\n",
    "batchSize=5 #if you have more gpu memory you may increase it and your training will be faster\n",
    "nClasses= len(alphabet)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "numLayers=2 # the 2 BLSTM layers defined seprately without using numLayers option for nn.LSTM\n",
    "numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "model = rnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "start = time.time()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "\n",
    "for iter in range (0,200):\n",
    "    avgTrainCost=0\n",
    "    random.shuffle(words)\n",
    "\n",
    "    for i in range (0,vocabSize-batchSize+1,batchSize):\n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        batchOfWords=words[i:i+batchSize]\n",
    "        images,labelSeqs,labelSeqlens =GetBatch(batchOfWords)\n",
    "        images=autograd.Variable(images)\n",
    "        #images=autograd.Variable(images)\n",
    "        images=images.contiguous()\n",
    "        if use_cuda:\n",
    "            images=images.cuda()\n",
    "        labelSeqs=autograd.Variable(labelSeqs)\n",
    "\n",
    "        labelSeqlens=autograd.Variable(labelSeqlens)\n",
    "        outputs=model(images)\n",
    "        outputs=outputs.contiguous()\n",
    "        outputsSize=autograd.Variable(torch.IntTensor([outputs.size(0)] * batchSize))\n",
    "        trainCost = criterion(outputs, labelSeqs, outputsSize, labelSeqlens) / batchSize\n",
    "\n",
    "        avgTrainCost+=trainCost\n",
    "        if i%500==0:\n",
    "            avgTrainCost=avgTrainCost/(5000/batchSize)\n",
    "            #print ('avgTraincost for last 5000 samples is',avgTrainCost)\n",
    "            avgTrainCost=0\n",
    "            valOutputs=model(valImages)\n",
    "#print (valOutputs.size()) 100 X nvalsamoles x 37\n",
    "            valOutputs=valOutputs.contiguous()\n",
    "            valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "            valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "            print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "            ### get the actual predictions and compute word error ################\n",
    "            valOutputs=valOutputs.transpose(0,1)\n",
    "            # second output of max() is the argmax along the requuired dimension\n",
    "            _, argMaxActivations= valOutputs.max(2)\n",
    "            #the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "            predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen\n",
    "            predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "            for ii in range(0,5):\n",
    "\n",
    "                print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n",
    "\n",
    "            #   print (predictedSeqLabels[0,:].transpose(0,0))\n",
    "            #print(valOutputs_batchFirst[0,0,:])\n",
    "            print('Time since we began trainiing [%s]' % (time_since(start)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        trainCost.backward()\n",
    "        optimizer.step()\n",
    "    #iterString=int(iter)\n",
    "    #torch.save(model.state_dict(), iterString+'.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 ###\n",
    "1. While the train cost is computed , what are the dimensions of the 4 arguments supplied to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained model and testing the validation data on it ###\n",
    "In case your networks takes lot of time to converge, we have a pretrained model for you. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 11.0239067078\n",
      "w~~~~~~~~~~d~~~~~~~~~d~~~~~~~~~6~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~f~~~~~~~d~~~~~~~~~~~~~~~~~~~~~~~~~~~~~==>wdd6fd\n",
      "~h~~~~~~~~~~~~~~~~y~~~~~d~~~~~~~~~~e~~~~~~~~~~r~~~~~~~o~~~~~~~~~~b~~~~~~~~~~~o~~~~~~~~~~d~~~~~~~~~~~==>hyderobod\n",
      "g~~~~~~~~~~~o~~~~~~~~~~~~~l~~~~~e~~~~~~~~~~~o~~~~~~~~~~~~~n~~~~~~~~~~~~~t~~~~~~~~l~~~~~e~~~~~~~~~~~~==>goleontle\n",
      "c~~~~~~~~~h~~~~~~~~~~~~a~~~~~~~~~~~r~~~~~~~m~~~~~~~~~~~~~~~~~~i~~~~~n~~~~~~~~~~~a~~~~~~~~~~~r~~~~~~~==>charminar\n",
      "g~~~~~~~~~~a~~~~~~~~~~~c~~~~~~~~~h~~~~~~~~~~~i~~~~~b~~~~~~~~~~~o~~~~~~~~w~~~~~~~~~~~~~~~l~~~~~i~~~~~==>gachibowli\n"
     ]
    }
   ],
   "source": [
    "#  load a saved model and test our test/validation data on it #\n",
    "use_cuda=True #since the saved model is a gpu model\n",
    "model = torch.load(\"../../../data/lab2/ocr_valE5_blstm.pt\")\n",
    "criterion = CTCLoss()\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "    \n",
    "\n",
    "#optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "#model.load_state_dict(torch.load(\"../../../data/lab2/ocrmodel_iter_40.pt\n",
    "\n",
    "\n",
    "valWords=['9446567456','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(valWords)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valOutputs=model(valImages)\n",
    "valOutputs=valOutputs.contiguous()\n",
    "valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "# valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "# second output of max() is the argmax along the requuired dimension\n",
    "_, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "#the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "#print the predicted raw string and the decoded string for the valimages\n",
    "for ii in range(0,5):\n",
    "\n",
    "    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 5 ####\n",
    "1. For the image of 'hyderabad', plot the output activations of all the classes for timestep=2(class labels along x axis and probabilities along y) and find out the probabilities for class h and b\n",
    "    - hint - do a softmax on the activations of its not already applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
