{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training an OCR using RNN + CTC on Synthetic Images ##\n",
    "- To train an RNN learn seq2seq mapping, when your input sequence and output sequence are not aligned\n",
    "- Input sequence is a sequence of image features and output is a sequence of characters\n",
    "- A word image's each column is treated as a timestep. so inputdim= height of the word image and seqlen= width of the image\n",
    "- Training images are rendered on the fly for the task\n",
    "- Images are resized to  fixed width , though we can have variying widths since RNN can handle variable length sequences. This helps in faster batch learning\n",
    "- We train a network with a bidirectional RNN  and a CTC loss for the task\n",
    "- To make sure the networks learns the mappings we first overfit it to 3 letter words\n",
    "- Then we will the train network on a larger dataset, comprising of images rendered from 90k English words\n",
    "- The network is tested on synthetic images, but rendered from out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Use a BRNN + CTC to recognize given word image \n",
    "# Network is trained on images rendered using PIL \n",
    "# ============================================================================\n",
    "# for ML Summer School 2017 at IIIT - HYD\n",
    "# Authors -minesh\n",
    "# Do not share this code or the associated exercises anywhere\n",
    "# we might be using the same code/ exercies for our future schools/ events\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import numpy as np\n",
    "import time,math\n",
    "from time import sleep\n",
    "import random\n",
    "import sys,codecs,glob \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "random.seed(0)\n",
    "# TODO - MAKE SURE CTC IS INSTALLED IN ALL MACHINES\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary and the fonts ####\n",
    "-  loading the lexicon of 90k words\n",
    "- get the fontslist to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the vocabulary = 88172\n",
      "number of fonts in the collection = 65\n"
     ]
    }
   ],
   "source": [
    "#all word images are resized to a height of 32 pixels\n",
    "imHeight=32 \n",
    "\"\"\"\n",
    "image width is also set a fixed size\n",
    "YES. Though RNNS can handle variable length sequences we resize them to fixed width\n",
    "This is for the ease of batch learning\n",
    "And it doesnt seem to affect the performance much atleast in our case\n",
    "\n",
    "Pytorch provides a packed array API incase you want to have variable length sequences within a batch\n",
    "see the discussion here\n",
    "https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120/8\n",
    "\n",
    "\"\"\"\n",
    "imWidth=100\n",
    "#imWidth=15\n",
    "#65 google fonts are used\n",
    "fontsList=glob.glob('../../../data/lab2/googleFonts/'+'*.ttf')\n",
    "#lexicon has 90k words\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n",
    "fontSizeOptions={'16','20','24','28','30','32','36','38'}\n",
    "\n",
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "#alphabet=\"(3)-\"\n",
    "dict={}\n",
    "for i, char in enumerate(alphabet):\n",
    "\tdict[char] = i + 1\n",
    "print('number of words in the vocabulary =', vocabSize)\n",
    "print('number of fonts in the collection =', len(fontsList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## a simple helper function to compute time since some 'start time'\n",
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return the class labels for each character in the targetsequence \n",
    "def Str2Labels(text):\n",
    "\tglobal dict\n",
    "\ttext = [dict[char.lower()] for char in text]\n",
    "\t#print (text)\n",
    "\tlength=len(text)\n",
    "\treturn text, length\n",
    "#StrtoLabels(\"0-1\")\n",
    "\n",
    "### from the predicted sequence of labels for an image, decode the string\n",
    "# function returns the rawstring and also the decoded string after removing blanks and duplicates\n",
    "\n",
    "#eg: if labelsequnce you get after an argmax on the output activation matris is  [12,12,0,0,15,0,15,15,0,0]\n",
    "#then your raw label string would be \"bb~~e~ee~~\" and the outputstring \"bee\"\n",
    "def Labels2Str(predictedLabelSequences):\n",
    "    bz=predictedLabelSequences.size(0)\n",
    "    predictedRawStrings=[]\n",
    "    predictedStrings=[]\n",
    "    for i in range(0,bz):\n",
    "        predictedRawString=\"\"\n",
    "        predictedString=\"\"\n",
    "        predictedLabelSeq=predictedLabelSequences.data[i,:]\n",
    "        prevId=1000 #just a large value which is not in the index \n",
    "        character=\"\"\n",
    "        character_raw=\"\"\n",
    "        for j in range (0, predictedLabelSeq.size(0)):\n",
    "            idx=predictedLabelSeq[j]\n",
    "            if (prevId != 1000 or prevId!=idx) :\n",
    "                if prevId!=idx:\n",
    "                    if idx==0:\n",
    "                        character_raw=\"~\"\n",
    "                        character=\"\"\n",
    "                    else:\n",
    "                        character_raw=alphabet[idx-1]\n",
    "                        character=alphabet[idx-1]\n",
    "                else:\n",
    "                    character_raw=\"~\"\n",
    "                    character=\"\"\n",
    "                prevId=idx\n",
    "            else:\n",
    "                character=\"\"\n",
    "                if idx==0:\n",
    "                    character_raw=\"~\"\n",
    "                else:\n",
    "                    character_raw=alphabet[idx-1]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            predictedString+=character\n",
    "            predictedRawString+=character_raw\n",
    "        predictedRawStrings.append(predictedRawString)\n",
    "        predictedStrings.append(predictedString)\n",
    "        \n",
    "    return predictedRawStrings, predictedStrings\n",
    "\n",
    "\n",
    "\n",
    "def image2tensor(im):\n",
    "    #returns the pixel values of a PIL image (in 0-1 range) as a numpy 2D array\n",
    "\n",
    "    (width, height) = im.size\n",
    "    greyscale_map = list(im.getdata())\n",
    "    greyscale_map = np.array(greyscale_map, dtype = np.uint8)\n",
    "    greyscale_map=greyscale_map.astype(float)\n",
    "    greyscale_map = torch.from_numpy(greyscale_map.reshape((height, width))).float()/255.0\n",
    "    return greyscale_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images, prepare a training batch ###\n",
    "- renders a batch of word images, from the list of words supplied\n",
    "- if singleFont is true then only one font would be used to render images. This is useful in case where you want to test overfitting the network to easy examples\n",
    "- Along with the rendered images, the target strings are converted to corresponding sequence of labels; for example the word \"bee\" would be converted to [12,15,15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetBatch ( imWidth,batchOfWords,singleFont ):\n",
    "    \"\"\"\n",
    "    Renders a batch of word images and returns the images along with the corresponding GTs\n",
    "    Uses PIL to render word images\n",
    "    font is randomly picked from a set of freely available google fonts\n",
    "    word is picked from a vocabulary of English words\n",
    "\n",
    "    \"\"\"\n",
    "    wordImages=[]\n",
    "    labelSequences=[]\n",
    "    labelSeqLengths=[]\n",
    "\n",
    "    for  i,text in enumerate (batchOfWords):\n",
    "        wordText=text\n",
    "        if singleFont==1:\n",
    "            fontName=fontsList[0]\n",
    "            fontSize='26'\n",
    "        else:\n",
    "            fontName=random.sample(fontsList,1)[0]\n",
    "            fontSize=random.sample(fontSizeOptions,1)[0] \n",
    "        imageFont = ImageFont.truetype(fontName,int(fontSize))\n",
    "        textSize=imageFont.getsize(wordText)\n",
    "        img=Image.new(\"L\", textSize,(255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((0, 0),wordText,(0),font=imageFont)\n",
    "        img=img.resize((imWidth,imHeight), Image.ANTIALIAS)\n",
    "        #img.save(text+'.jpeg')\n",
    "\n",
    "        imgTensor=image2tensor(img)\n",
    "        imgTensor=imgTensor.unsqueeze(0) # at 0 a new dimenion is added\n",
    "\n",
    "        wordImages.append(imgTensor)\n",
    "\n",
    "        labelSeq,l=Str2Labels(wordText)\n",
    "        labelSequences+=labelSeq\n",
    "        labelSeqLengths.append(l)\n",
    "    batchImageTensor=torch.cat(wordImages,0) #BxHxW\n",
    "    batchImageTensor=batchImageTensor.unsqueeze(1) # BxCxHxW\n",
    "    #now all the image tensors are combined ( we  did the unsqueeze eariler for this cat)  \n",
    "    #batchImageTensor=torch.transpose(batchImageTensor,1,2)\n",
    "    labelSequencesTensor=torch.IntTensor(labelSequences)\n",
    "    labelSeqLengthsTensor=torch.IntTensor(labelSeqLengths)\n",
    "    return batchImageTensor, labelSequencesTensor, labelSeqLengthsTensor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defintion  ###\n",
    "![OCR Architecture](blstm.jpg)\n",
    "- Input image here is of shape 100*32. Hence seqLen=100 and your featDim at a timestep =32\n",
    "- The below network has two BLSTM layers with #neurons in each layer = hiddenDim\n",
    "- the outputs of both the forward and backward recurrent layers in the second hidden layer are connected to a linear layer. There are hiddenDim*2 connections coming to this layer and its output is of size=outputDim=nClasses+1 (one extra class for blank label of CTC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minesh TODO split blstm into a separate class ?\n",
    "\n",
    "class rnnocr (nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim,  numLayers, numDirections):\n",
    "        super(rnnocr, self).__init__()\n",
    "        self.inputDim=inputDim\n",
    "        self.hiddenDim=hiddenDim\n",
    "        self.outputDim=outputDim\n",
    "        self.numLayers=numLayers\n",
    "        self.numDirections=numDirections\n",
    "        # bidirectional= true to make the rnn bidirectional\n",
    "        #cnn stack\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        \n",
    "        \n",
    "        # rnn part\n",
    "        self.blstm1=nn.LSTM(384, hiddenDim,numLayers, bidirectional=True, batch_first=True) # first blstm layer takes the image features as inputs\n",
    "                \n",
    "        self.linearLayer2=nn.Linear(hiddenDim*numDirections, outputDim) # linear layer at the output\n",
    "        self.softmax = nn.Softmax()\n",
    "                \n",
    "    def forward(self, x ):\n",
    "        #print(x.size())\n",
    "        B,C,T,D=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #print('size of x in the beginning =', x.size()) # batxhSizexnumChannels=1xHxW\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # batchSizex64xH/2-1(W/2-1)x\n",
    "        #print('size of x after conv1 and pooling =', x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # batchSizex64xH/2/2-1x(W/2-1)/2-1\n",
    "        #print('size of x after conv2 and pooling =', x.size())\n",
    "        #if input is 50x1x32x100 then it would become 50x64x16*49 and then 50x64x6x23\n",
    "        #print(x.size())\n",
    "        x=x.contiguous()\n",
    "        B,C,D,T=x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "        #x=x.transpose(2,3) #swapping last two dimensions\n",
    "        x=x.contiguous()\n",
    "        x=x.view(B,x.size(1)*x.size(2),-1) # BxC*DXT\n",
    "        x=x.transpose(1,2) #making T the second dimension\n",
    "        #print(x.size())\n",
    "        \n",
    "        \n",
    "        lstmOut1, _  =self.blstm1(x ) #x has three dimensions batchSize* seqLen * FeatDim\n",
    "        B,T,D  = lstmOut1.size(0), lstmOut1.size(1), lstmOut1.size(2)\n",
    "        lstmOut1=lstmOut1.contiguous()\n",
    "\n",
    "                \n",
    "        # output of RNN is reshaped to B*T x D before it is fed to the linear layer\n",
    "        outputLayerActivations=self.linearLayer2(lstmOut1.view(B*T,D))\n",
    "        outputSoftMax=self.softmax(outputLayerActivations)\n",
    "        # the activations are reshaped to B x T x outputDim size\n",
    "        #then a transpose of B and T since CTC expects the T to be first\n",
    "        outputLayerActivations= outputLayerActivations.view(B,T,-1).transpose(0,1)\n",
    "        #if use_cuda:\n",
    "        #    outputLayerActivations=outputLayerActivations.cuda()\n",
    "        return outputLayerActivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,singleFont, saveTrue ):\n",
    "    batchSize=30\n",
    "    nHidden=80\n",
    "    nClasses= len(alphabet)\n",
    "    criterion = CTCLoss()\n",
    "    numLayers=2# \n",
    "    numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "    model = rnnocr(imHeight,nHidden,nClasses,numLayers,numDirections)\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "        criterion=criterion.cuda()\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start = time.time()\n",
    "    for iter in range (0,200):\n",
    "        avgTrainCost=0\n",
    "        random.shuffle(words)\n",
    "        \n",
    "        for i in range (0,vocabSize-batchSize+1,batchSize):\n",
    "\n",
    "            model.zero_grad()\n",
    "            #words which need to be rendered into images are sequentially taken from the lexicon\n",
    "            #the number of words rendered at a time = batchSize\n",
    "            batchOfWords=words[i:i+batchSize]\n",
    "            # GetBatch() returns the rendered images, the labelseq(GT) for each image and the lengths of each of the labelseq\n",
    "            images,labelSeqs,labelSeqlens =GetBatch(imWidth,batchOfWords,singleFont)\n",
    "            images=autograd.Variable(images)\n",
    "            # coniguous since we will be doing a view() of this later\n",
    "            images=images.contiguous()\n",
    "            labelSeqs=autograd.Variable(labelSeqs)\n",
    "            labelSeqlens=autograd.Variable(labelSeqlens)\n",
    "\n",
    "            if use_cuda:\n",
    "                images=images=images.cuda()\n",
    "            #do the forward pass\n",
    "            outputs=model(images)\n",
    "            outputs=outputs.contiguous()\n",
    "            #the size of the output activations, this is required when you call the CTC loss\n",
    "            outputsSize=autograd.Variable(torch.IntTensor([outputs.size(0)] * batchSize))\n",
    "            trainCost = criterion(outputs, labelSeqs, outputsSize, labelSeqlens) / batchSize\n",
    "\n",
    "            avgTrainCost+=trainCost\n",
    "            if i%10000==0:\n",
    "                avgTrainCost=avgTrainCost/(10000/batchSize)\n",
    "                #print ('avgTraincost for last 5000 samples is',avgTrainCost)\n",
    "                avgTrainCost=0\n",
    "                # forward the network with the validation iamges as input\n",
    "                valOutputs=model(valImages)\n",
    "                #print (valOutputs.size()) 100 X nvalsamoles x 37\n",
    "                valOutputs=valOutputs.contiguous()\n",
    "                valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "                valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "                print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "                # valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "                valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "                # second output of max() is the argmax along the requuired dimension\n",
    "                _, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "                #the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "                predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "                predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "                #print the predicted raw string and the decoded string for the valimages\n",
    "                for ii in range(0,5):\n",
    "\n",
    "                    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n",
    "                    #print(predictedStrings[ii])\n",
    "\n",
    "                    #   print (predictedSeqLabels[0,:].transpose(0,0))\n",
    "                #print(valOutputs_batchFirst[0,0,:])\n",
    "                #print (argMaxActivations[0,:])\n",
    "                print('Time since we began trainiing [%s]' % (time_since(start)))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            trainCost.backward()\n",
    "            optimizer.step()\n",
    "        print( 'completed  iteration no -', iter)\n",
    "        if (iter%2==0 or valCost.data[0]  < 4   ) and saveTrue :\n",
    "            iterString=str(iter)\n",
    "            torch.save(model, 'ocrmodel_iter_'+iterString+'.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### now lets try on a larger data set, which is rendered from a large vocabulary of 90k words ##\n",
    "vocabFile=codecs.open('../../../data/lab2/lexicon.txt','r')\n",
    "words = vocabFile.read().split()\n",
    "vocabSize=len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Prepare the synthetic validation data for the training\n",
    "##############\n",
    "imWidth=100\n",
    "valWords=['944678567','hyderabad','golconda','charminar','gachibowli']\n",
    "valImages, valLabelSeqs, valLabelSeqlens=GetBatch(imWidth,valWords,1)\n",
    "valImages=autograd.Variable(valImages)\n",
    "valImages=valImages.contiguous()\n",
    "\n",
    "    \n",
    "valLabelSeqs=autograd.Variable(valLabelSeqs)\n",
    "#print(valLabelSeqs.data)\n",
    "valLabelSeqlens=autograd.Variable(valLabelSeqlens)\n",
    "if use_cuda:\n",
    "    valImages=valImages.cuda()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validaton Cost is 63.7468147278\n",
      "q~~~~~~~~~~~~~~~~~~~~u~==>qu\n",
      "q~~~~~~~~~~~~~~~~~~~~u~==>qu\n",
      "q~~~~~~~~~~~~~~~~~~~~u~==>qu\n",
      "q~~~~~~~~~~~~~~~~~~~~u~==>qu\n",
      "q~~~~~~~~~~~~~~~~~~~~u~==>qu\n",
      "Time since we began trainiing [0m 0s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /OCRData/minesh.mathew/pytorch-0.1.12/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e5059e7951aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainNtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimWidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalLabelSeqlens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# set the second last argument in above function call to 1 if it trains more than 10 minutes to converge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# if the second last argument is 1 then only one font will be used in rendering images and so it ll converge fast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#the last argument =1 means the models will be saved at regular intervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b1366c17c4be>\u001b[0m in \u001b[0;36mtrainNtest\u001b[1;34m(imWidth, valImages, valLabelSeqs, valLabelSeqlens, singleFont, saveTrue)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mtrainCost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'completed  iteration no -'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[0;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/_functions/linear.pyc\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mgrad_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mgrad_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /OCRData/minesh.mathew/pytorch-0.1.12/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "trainNtest(imWidth,valImages, valLabelSeqs, valLabelSeqlens,0,1)\n",
    "# set the second last argument in above function call to 1 if it trains more than 10 minutes to converge\n",
    "# if the second last argument is 1 then only one font will be used in rendering images and so it ll converge fast\n",
    "#the last argument =1 means the models will be saved at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained model and testing the validation data on it ###\n",
    "In case your networks takes lot of time to converge, we have a pretrained model for you. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  load a saved model and test our test/validation data on it #\n",
    "torch.__file__\n",
    "batchSize=50\n",
    "nHidden=80\n",
    "nClasses= len(alphabet)\n",
    "criterion = CTCLoss()\n",
    "numLayers=2# \n",
    "numDirections=2 # 2 since we need to use a bidirectional LSTM\n",
    "#we are loding the entire model, not just the state here\n",
    "model = torch.load(\"../../../data/lab2/ocr_valE5_blstm.pt\")\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "#model.load_state_dict(torch.load(\"../../../data/lab2/ocrmodel_iter_40.pt\n",
    "\n",
    "valOutputs=model(valImages)\n",
    "valOutputs=valOutputs.contiguous()\n",
    "valOutputsSize=autograd.Variable(torch.IntTensor([valOutputs.size(0)] * len(valWords)))\n",
    "valCost=criterion(valOutputs, valLabelSeqs, valOutputsSize, valLabelSeqlens) / len(valWords)\n",
    "print ('validaton Cost is',valCost.data[0])\n",
    "\n",
    "\n",
    "# valOutputs is in TxBxoutputDim size we make it BxTxoutputDIm\n",
    "valOutputs_batchFirst=valOutputs.transpose(0,1)\n",
    "# second output of max() is the argmax along the requuired dimension\n",
    "_, argMaxActivations= valOutputs_batchFirst.max(2)\n",
    "#the below tensor each raw is the sequences of labels predicted for each sample in the batch\n",
    "predictedSeqLabels=argMaxActivations.squeeze(2) #batchSize * seqLen \n",
    "predictedRawStrings,predictedStrings=Labels2Str(predictedSeqLabels)\n",
    "#print the predicted raw string and the decoded string for the valimages\n",
    "for ii in range(0,5):\n",
    "\n",
    "    print (predictedRawStrings[ii]+\"==>\"+predictedStrings[ii])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
