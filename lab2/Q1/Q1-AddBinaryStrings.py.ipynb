{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to add two binary strings ##\n",
    "if  two input  binary strings say 010 and 011 are given your network should output the sum = 101\n",
    "\n",
    "- How do you represent the data \n",
    "- Defining a simple recurrent net model for the task\n",
    "- Try with vanilla -RNNs fist and see how it performs\n",
    "- What would you change in the network if your vanilla RNN failed to give good results?\n",
    "- Train it on binary strings of a fixed length\n",
    "- Test the network using binary strings of different lengths\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# =============================================================================\n",
    "# Make a simple RNN learn binray addition \n",
    "# Binary string pairs and the sum is generated for a given #numBits\n",
    "# ============================================================================\n",
    "# for ML Summer School 2017 at IIIT - HYD\n",
    "# Author- minesh\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# TODO -minesh  does more hidden units help ?\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "random.seed( 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data ##\n",
    "\n",
    "###   Radom binary strings of required length as training data ###\n",
    "The  function <i>getSample()</i> takes a string-length as input and then returns the input vector and target vector that need to be fed to the RNN\n",
    "Say if your string-length is 2, lower and upper bounds would be 2 and 3. \n",
    "Then if the two random numbers picked from this range are 2 and 3 ( you have only 2 and 3 in that range :) )\n",
    "your inputs in binary would be 10 and 11 and your sum is 5 which is 101.\n",
    "Since your ouput is one bit longer we will rewrite the inputs too in 3 bit form so  010 + 011 -- > 101\n",
    "\n",
    "\n",
    "### How do i represent the data  ###\n",
    "Starting from the least significant bit  (since the addition starts from LSB) we concatenate the correspodning bits in each input binary string and that forms our input sequence.\n",
    "And your target vector would be the ourput binary string reversed (Since you start from LSB)\n",
    "\n",
    "Hence  your input at one timestep is this ordered pair of bits for that particular position and target for that timestep would be the corresponding bit in the output string\n",
    "\n",
    "so your input dimension at each time step is 2 and target dimesnion is 1\n",
    "\n",
    "in the above case so your input and target pairs would be\n",
    "\n",
    "[1 0] - > 1\n",
    "[1 1]  -> 0\n",
    "[0 0]  -> \n",
    "would be [1 0] , [1 1] , [0 0] and output is [1 0 1]\n",
    "\n",
    "![title](binaryinput.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSample(stringLength, testFlag):\n",
    "\t#takes stringlength as input \n",
    "\t#returns a sample for the network - an input sequence - x and its target -y\n",
    "\t#x is a T*2 array, T is the length of the string and 2 since we take one bit each from each string\n",
    "\t#testFlag if set prints the input numbers and its sum in both decimal and binary form\n",
    "\tlowerBound=pow(2,stringLength-1)+1\n",
    "\tupperBound=pow(2,stringLength)\n",
    "\n",
    "\tnum1=random.randint(lowerBound,upperBound)\n",
    "\tnum2=random.randint(lowerBound,upperBound)\n",
    "\n",
    "\tnum3=num1+num2\n",
    "\tnum3Binary=(bin(num3)[2:])\n",
    "\n",
    "\tnum1Binary=(bin(num1)[2:])\n",
    "\n",
    "\tnum2Binary=(bin(num2)[2:])\n",
    "\n",
    "\tif testFlag==1:\n",
    "\t\tprint('input numbers and their sum  are', num1, ' ', num2, ' ', num3)\n",
    "\t\tprint ('binary strings are', num1Binary, ' ' , num2Binary, ' ' , num3Binary)\n",
    "\tlen_num1= (len(num1Binary))\n",
    "\n",
    "\tlen_num2= (len(num2Binary))\n",
    "\tlen_num3= (len(num3Binary))\n",
    "\n",
    "\t# since num3 will be the largest, we pad  other numbers with zeros to that num3_len\n",
    "\tnum1Binary= ('0'*(len(num3Binary)-len(num1Binary))+num1Binary)\n",
    "\tnum2Binary= ('0'*(len(num3Binary)-len(num2Binary))+num2Binary)\n",
    "\n",
    "\n",
    "\t# forming the input sequence\n",
    "\t# the input at first timestep is the least significant bits of the two input binary strings\n",
    "\t# x will be then a len_num3 ( or T ) * 2 array\n",
    "\tx=np.zeros((len_num3,2),dtype=np.float32)\n",
    "\tfor i in range(0, len_num3):\n",
    "\t\tx[i,0]=num1Binary[len_num3-1-i] # note that MSB of the binray string should be the last input along the time axis\n",
    "\t\tx[i,1]=num2Binary[len_num3-1-i]\n",
    "\t# target vector is the sum in binary\n",
    "\t# convert binary string in <string> to a numpy 1D array\n",
    "\t#https://stackoverflow.com/questions/29091869/convert-bitstring-string-of-1-and-0s-to-numpy-array\n",
    "\ty=np.array(map(int, num3Binary[::-1]))\n",
    "\t#print (x)\n",
    "\t#print (y)\n",
    "\treturn x,y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the network look like ? ##\n",
    "The figure below shows  fully rolled network for our task for the input - target pair we took as an example earlier.\n",
    "In the figure, for ease of drawing, hiddenDIm is chosen as 2\n",
    "![network architecture](binarynet.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adder (nn.Module):\n",
    "\tdef __init__(self, inputDim, hiddenDim, outputDim):\n",
    "\t\tsuper(Adder, self).__init__()\n",
    "\t\tself.inputDim=inputDim\n",
    "\t\tself.hiddenDim=hiddenDim\n",
    "\t\tself.outputDim=outputDim\n",
    "\t\tself.lstm=nn.RNN(inputDim, hiddenDim )\n",
    "\t\tself.outputLayer=nn.Linear(hiddenDim, outputDim)\n",
    "\t\tself.sigmoid=nn.Sigmoid()\n",
    "\tdef forward(self, x ):\n",
    "\t\t#size of x is T x B x featDim\n",
    "\t\t#B=1 is dummy batch dimension added, because pytorch mandates it\n",
    "\t\t#if you want B as first dimension of x then specift batchFirst=True when LSTM is initalized\n",
    "\t\t#T,D  = x.size(0), x.size(1)\n",
    "\t\t#batch is a must \n",
    "\t\tlstmOut,_ =self.lstm(x ) #x has two  dimensions  seqLen *batch* FeatDim=2\n",
    "\t\tT,B,D  = lstmOut.size(0),lstmOut.size(1) , lstmOut.size(2)\n",
    "\t\tlstmOut = lstmOut.contiguous() \n",
    "        # before  feeding to linear layer we squash one dimension\n",
    "\t\tlstmOut = lstmOut.view(B*T, D)\n",
    "\t\toutputLayerActivations=self.outputLayer(lstmOut)\n",
    "\t\t#reshape actiavtions to T*B*outputlayersize\n",
    "\t\toutputLayerActivations=outputLayerActivations.view(T,B,-1).squeeze(1)\n",
    "\t\toutputSigmoid=self.sigmoid(outputLayerActivations)\n",
    "\t\treturn outputSigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traning the network ###\n",
    "\n",
    "- batch learning is not used, only one seqeuence is fed at a time\n",
    "- runs purely on a cpu\n",
    "- MSE loss is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      " Avg. Loss for last 500 samples = inf\n",
      " Avg. Loss for last 500 samples = 0.183431\n",
      " Avg. Loss for last 500 samples = 0.175901\n",
      " Avg. Loss for last 500 samples = 0.173191\n",
      " Avg. Loss for last 500 samples = 0.171829\n",
      " Avg. Loss for last 500 samples = 0.165327\n",
      " Avg. Loss for last 500 samples = 0.173916\n",
      " Avg. Loss for last 500 samples = 0.173361\n",
      " Avg. Loss for last 500 samples = 0.168028\n",
      " Avg. Loss for last 500 samples = 0.167956\n",
      " Avg. Loss for last 500 samples = 0.166152\n",
      " Avg. Loss for last 500 samples = 0.160349\n",
      " Avg. Loss for last 500 samples = 0.157556\n",
      " Avg. Loss for last 500 samples = 0.148872\n",
      " Avg. Loss for last 500 samples = 0.141928\n",
      " Avg. Loss for last 500 samples = 0.133639\n",
      " Avg. Loss for last 500 samples = 0.118589\n",
      " Avg. Loss for last 500 samples = 0.104341\n",
      " Avg. Loss for last 500 samples = 0.083448\n",
      " Avg. Loss for last 500 samples = 0.062745\n",
      " Avg. Loss for last 500 samples = 0.048126\n",
      " Avg. Loss for last 500 samples = 0.034521\n",
      " Avg. Loss for last 500 samples = 0.023812\n",
      " Avg. Loss for last 500 samples = 0.016444\n",
      " Avg. Loss for last 500 samples = 0.011685\n",
      " Avg. Loss for last 500 samples = 0.007691\n",
      " Avg. Loss for last 500 samples = 0.005278\n",
      " Avg. Loss for last 500 samples = 0.003757\n",
      " Avg. Loss for last 500 samples = 0.002603\n",
      " Avg. Loss for last 500 samples = 0.001906\n",
      " Avg. Loss for last 500 samples = 0.001378\n",
      " Avg. Loss for last 500 samples = 0.001005\n",
      " Avg. Loss for last 500 samples = 0.000746\n",
      " Avg. Loss for last 500 samples = 0.000536\n",
      " Avg. Loss for last 500 samples = 0.000418\n",
      " Avg. Loss for last 500 samples = 0.000312\n",
      " Avg. Loss for last 500 samples = 0.000237\n",
      " Avg. Loss for last 500 samples = 0.000181\n",
      " Avg. Loss for last 500 samples = 0.000135\n",
      " Avg. Loss for last 500 samples = 0.000103\n"
     ]
    }
   ],
   "source": [
    "featDim=2 # two bits each from each of the String\n",
    "outputDim=1 # one output node which would output a zero or 1\n",
    "\n",
    "lstmSize=20\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "model =Adder(featDim, lstmSize, outputDim)\n",
    "print ('model initialized')\n",
    "#optimizer = optim.SGD(model.parameters(), lr=3e-2, momentum=0.8)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs=500\n",
    "### epochs ##\n",
    "totalLoss= float(\"inf\")\n",
    "while totalLoss > 1e-4:\n",
    "\tprint(\" Avg. Loss for last 500 samples = %lf\"%(totalLoss))\n",
    "\ttotalLoss=0\n",
    "\tfor i in range(0,epochs):\n",
    "\t\t\n",
    "\t\tstringLen=3\n",
    "\t\ttestFlag=0\n",
    "\t\tx,y=getSample(stringLen, testFlag)\n",
    "\n",
    "\t\tmodel.zero_grad()\n",
    "\n",
    "\n",
    "\t\tx_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float()) #convert to torch tensor and variable\n",
    "\t\t# unsqueeze() is used to add the extra dimension since\n",
    "\t\t# your input need to be of t*batchsize*featDim; you cant do away with the batch in pytorch\n",
    "\t\tseqLen=x_var.size(0)\n",
    "\t\t#print (x_var)\n",
    "\t\tx_var= x_var.contiguous()\n",
    "\t\ty_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "\t\tfinalScores = model(x_var)\n",
    "\t\t#finalScores=finalScores.\n",
    "\n",
    "\t\tloss=lossFunction(finalScores,y_var)\t\n",
    "\t\ttotalLoss+=loss.data[0]\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\t\n",
    "\ttotalLoss=totalLoss/epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model ###\n",
    "Remember that the network was purely trained on strings of length =3 <br>\n",
    "now lets the net on bitstrings of length=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input numbers and their sum  are 16   16   32\n",
      "binary strings are 10000   10000   100000\n",
      "sum predicted by RNN is  [1 0 0 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 11   15   26\n",
      "binary strings are 1011   1111   11010\n",
      "sum predicted by RNN is  [1 1 1 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 11   11   22\n",
      "binary strings are 1011   1011   10110\n",
      "sum predicted by RNN is  [1 0 1 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 14   10   24\n",
      "binary strings are 1110   1010   11000\n",
      "sum predicted by RNN is  [1 1 1 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 16   15   31\n",
      "binary strings are 10000   1111   11111\n",
      "sum predicted by RNN is  [1 1 1 1 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 13   13   26\n",
      "binary strings are 1101   1101   11010\n",
      "sum predicted by RNN is  [1 1 0 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 9   13   22\n",
      "binary strings are 1001   1101   10110\n",
      "sum predicted by RNN is  [1 0 1 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 13   15   28\n",
      "binary strings are 1101   1111   11100\n",
      "sum predicted by RNN is  [1 1 1 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 13   16   29\n",
      "binary strings are 1101   10000   11101\n",
      "sum predicted by RNN is  [1 1 1 0 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 11   12   23\n",
      "binary strings are 1011   1100   10111\n",
      "sum predicted by RNN is  [1 0 1 1 1]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "stringLen=4\n",
    "testFlag=1\n",
    "for i in range (0,10):\n",
    "\tx,y=getSample(stringLen,testFlag)\n",
    "\tx_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float())\n",
    "\ty_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "\tseqLen=x_var.size(0)\n",
    "\tx_var= x_var.contiguous()\n",
    "\tfinalScores = model(x_var).data.t()\n",
    "\t#print(finalScores)\n",
    "\tbits=finalScores.gt(0.5)\n",
    "\tbits=bits[0].numpy()\n",
    "\t#print(bits)\t\n",
    "\t#inverting a 1d tensor in tensor is not easy !!\n",
    "\t#https://github.com/pytorch/pytorch/issues/229\n",
    "\t#inv_idx = torch.arange(bits.size(0)-1, -1, -1).long()\n",
    "\t#inv_tensor = bits.index_select(0, inv_idx)\n",
    "\t#bits_inverted= bits[inv_index]\n",
    "\t\n",
    "\tprint ('sum predicted by RNN is ',bits[::-1])\n",
    "\tprint('##################################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
