{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to add two binary strings ##\n",
<<<<<<< HEAD
    "\n",
    "In this first lab, to get familiar with RNNs, we will explore the simple problem of teaching an RNN to add binary strings. Recall, like grade-school addition, binary addition moves from the right-most bit (least-significant bit or LSB) towards the left-most bit (most-significant) bit, with a carry bit passed from the previous addition.\n",
    "\n",
    "Following is the \"truth table\" for a \"full-adder\" (i.e., carry-in, carry-out):\n",
    "\n",
    "     i1   i2   carry-in  |  sum  carry-out\n",
    "     --------------------+----------------\n",
    "     0    0       0      |   0      0\n",
    "     0    0       1      |   1      0\n",
    "     0    1       0      |   1      0\n",
    "     0    1       1      |   0      1\n",
    "     1    0       0      |   1      0\n",
    "     1    0       1      |   0      1\n",
    "     1    1       0      |   0      1\n",
    "     1    1       1      |   1      1\n",
    "\n",
    "where, `i1` and `i2` are the input bits\n",
    "\n",
    "The RNN is fed two bit-sequences and the target \"sum\" sequence.\n",
    "The sequence is ordered from LSB to MSB, i.e., time-step 1 (t=1) corresponds to LSB, and the last time-step is the MSB.\n",
    "\n",
    "For example:\n",
    "If the bit strings 010 (integer value = 2) and 011 (integer value = 3) are to be added to produce the sum 101 (integer value 5), the following is the sequence of inputs and targets to the RNN when training:\n",
    "\n",
    "    time | i1  i2 | output\n",
    "    -----+--------+--------\n",
    "     1   | 0    1 |   1\n",
    "     2   | 1    1 |   0\n",
    "     3   | 0    0 |   1\n",
    "\n",
    "Note, in the example above, the \"carry\" bit is not explicitly provided as the input, and the RNN has to *learn* the concept of a carry-bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview ###\n",
    "We will be using [PyTorch](http://pytorch.org) for implementation.\n",
    "\n",
    "This question is planned as below:\n",
    "\n",
    "    1. First we discuss how the training samples are generated\n",
    "    2. Next, the we discuss the input and output format used for the RNN\n",
    "    3. We set up the RNN network \n",
    "    4. We explore the effects of various parameters."
=======
    "if  two input  binary strings say 010 and 011 are given your network should output the sum = 101\n",
    "\n",
    "- How do you represent the data \n",
    "- Defining a simple recurrent net model for the task\n",
    "- Try with vanilla -RNNs fist and see how it performs\n",
    "- What would you change in the network if your vanilla RNN failed to give good results?\n",
    "- Train it on binary strings of a fixed length\n",
    "- Test the network using binary strings of different lengths\n",
    "- \n"
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
=======
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# =============================================================================\n",
    "# Make a simple RNN learn binray addition \n",
    "# Binary string pairs and the sum is generated for a given #numBits\n",
    "# ============================================================================\n",
    "# for ML Summer School 2017 at IIIT - HYD\n",
    "# Author- minesh\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# import various modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "random.seed( 10 ) # set the random seed (for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Training Data ##\n",
    "\n",
    "###   Radom binary strings of required length as training data ###\n",
<<<<<<< HEAD
    "The function `getSample` below takes a string-length `L` as input and returns a training sample to be fed to the RNN.\n",
=======
    "The  function <i>getSample()</i> takes a string-length as input and then returns the input vector and target vector that need to be fed to the RNN\n",
    "Say if your string-length is 2, lower and upper bounds would be 2 and 3. \n",
    "Then if the two random numbers picked from this range are 2 and 3 ( you have only 2 and 3 in that range :) )\n",
    "your inputs in binary would be 10 and 11 and your sum is 5 which is 101.\n",
    "Since your ouput is one bit longer we will rewrite the inputs too in 3 bit form so  010 + 011 -- > 101\n",
    "\n",
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
    "\n",
    "\n",
    "For a given length `L`, a training sample is a 2-tuple of (`input`, `output`), where\n",
    "\n",
    "* `input` is a tensor of size [`L+1x2`]:<br>\n",
    "\t* The second dimension of 2, corresponds to 2 inputs which are\n",
    "\t\tto be summed together.\n",
    "\t* The first row is for LSBs, and the last row correspond to MSBs.\n",
    "\t* The bit-strings are `L+1` due to a possible carry when adding two `L`-length bit strings.\n",
    "\n",
<<<<<<< HEAD
    "* `output` is a tensor of size `L+1`, which is the sum of the inputs\n"
=======
    "in the above case so your input and target pairs would be\n",
    "\n",
    "[1 0] - > 1\n",
    "[1 1]  -> 0\n",
    "[0 0]  -> \n",
    "would be [1 0] , [1 1] , [0 0] and output is [1 0 1]\n",
    "\n",
    "![title](binaryinput.jpg)\n",
    "\n"
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {},
=======
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
   "outputs": [],
   "source": [
    "def getSample(stringLength, testFlag=False):\n",
    "    \"\"\"\n",
    "    Returns a random sample for bit-string addition.\n",
    "    STRINGLENGTH: (int scalar) (one less than) length of the bit-string to return.\n",
    "    TESTFLAG: (boolean) if True, the returned sample is printed.\n",
    "    \n",
    "    Returns:\n",
    "        a 2-tuple of (Input,Output), where:\n",
    "        INPUT: (L+1 x 2) dimensional tensor of the inputs, where L==STRINGLENGTH\n",
    "        OUTPUT: (L+1) dimensional \"target\" vector, which is the binary sum of inputs.\n",
    "    \"\"\"\n",
    "    lowerBound=pow(2,stringLength-1)+1\n",
    "    upperBound=pow(2,stringLength)\n",
    "    \n",
    "    num1=random.randint(lowerBound,upperBound)\n",
    "    num2=random.randint(lowerBound,upperBound)\n",
    "\n",
    "    num3=num1+num2\n",
    "    num3Binary=(bin(num3)[2:])\n",
    "\n",
    "    num1Binary=(bin(num1)[2:])\n",
    "    num2Binary=(bin(num2)[2:])\n",
    "\n",
    "    if testFlag==1:\n",
    "        print('input numbers and their sum  are', num1, ' ', num2, ' ', num3)\n",
    "        print ('binary strings are', num1Binary, ' ' , num2Binary, ' ' , num3Binary)\n",
    "    len_num1= (len(num1Binary))\n",
    "\n",
    "    len_num2= (len(num2Binary))\n",
    "    len_num3= (len(num3Binary))\n",
    "\n",
    "    # since num3 will be the largest, we pad  other numbers with zeros to that num3_len\n",
    "    num1Binary= ('0'*(len(num3Binary)-len(num1Binary))+num1Binary)\n",
    "    num2Binary= ('0'*(len(num3Binary)-len(num2Binary))+num2Binary)\n",
    "\n",
    "    # forming the input sequence\n",
    "    # the input at first timestep is the least significant bits of the two input binary strings\n",
    "    # x will be then a len_num3 ( or T ) * 2 array\n",
    "    x=np.zeros((len_num3,2),dtype=np.float32)\n",
    "    for i in range(0, len_num3):\n",
    "        x[i,0]=num1Binary[len_num3-1-i] # note that MSB of the binray string should be the last input along the time axis\n",
    "        x[i,1]=num2Binary[len_num3-1-i]\n",
    "    # target vector is the sum in binary\n",
    "    # convert binary string in <string> to a numpy 1D array\n",
    "    #https://stackoverflow.com/questions/29091869/convert-bitstring-string-of-1-and-0s-to-numpy-array\n",
    "    y=np.array(map(int, num3Binary[::-1]))\n",
    "    return x,y \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input ###\n",
    "As noted above, the inputs are `L+1x2` dimensional tensors, with the first row corresponding to LSB and the last to MSB. As addition proceeds from LSB to MSB, the `input` rows are fed  one-by-one, starting from the top, proceeding all the way to the last row.<br>\n",
    "__Note__: This is not shown explicitly in the provided code, but is done internally.\n",
    "\n",
    "\n",
    "### Model Output  ###\n",
    "At each time-step the model needs to predict the target \"sum\".\n",
    "We do an affine projection (linear + bias) of the RNN hidden state at each time step to a single value $s_t$:\n",
    "\n",
    "$$s_t = W_{hy}h_t + b_{hy}$$\n",
    "\n",
    "Since, this output value $s_t$ is unconstrained, and we need a value in the set $\\{0,1\\}$, we use the `sigmoid` function to squash $s_t$ to the $[0,1]$ range:\n",
    "\n",
    "$$y_t = \\operatorname{sigmoid}(s_t) = \\frac{1}{1+e^{-s_t}}$$\n",
    "\n",
    "## Training Loss ##\n",
    "We use the `Mean-Squared-Error` loss function to compare the predicted value $o_t$, and the target $\\tilde{y}_t$:\n",
    "\n",
    "$$\\mathcal{L}_t(s_t,y_t) = \\left(s_t - \\tilde{y}_t\\right)^2$$\n",
    "\n",
    "__Note__: The total loss for a given sequence is the sum of all the losses from each time-step.\n",
    "\n",
    "\n",
    "The image below shows a schematic of the \"unrolled\" RNN for binary-addition:\n",
    "![network architecture](binAdd.png)\n",
    "\n",
    "\n",
    "## Model Implementation ##\n",
    "The following class `Adder` implements the above RNN. We only give the forward-pass implementation.\n",
    "The backward pass is calcuated automatically by PyTorch's auto-grad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adder (nn.Module):\n",
    "    def __init__(self,stateDim,rnn_cell='LSTM'):\n",
    "        super(Adder, self).__init__()\n",
    "        self.stateDim = stateDim\n",
    "        self.inputDim = 2  # two for the two inputs\n",
    "        self.outputDim = 2  # two for probabilities for `0` and `1`\n",
    "        self.lstm = nn.LSTM(self.inputDim, self.stateDim )\n",
    "        self.outputLayer = nn.Linear(self.stateDim, self.outputDim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X: [L,B,inputDim(=2)] dimensional input tensor\n",
    "            L: Sequence length\n",
    "            B: is the \"batch\" dimension. As we are training on \n",
    "               single examples, B = 1 for us.\n",
    "        \"\"\"\n",
    "        lstmOut,_ = self.lstm(x)\n",
    "        L,B,D  = lstmOut.size(0),lstmOut.size(1),lstmOut.size(2)\n",
    "        lstmOut = lstmOut.contiguous() \n",
=======
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adder (nn.Module):\n",
    "\tdef __init__(self, inputDim, hiddenDim, outputDim):\n",
    "\t\tsuper(Adder, self).__init__()\n",
    "\t\tself.inputDim=inputDim\n",
    "\t\tself.hiddenDim=hiddenDim\n",
    "\t\tself.outputDim=outputDim\n",
    "\t\tself.lstm=nn.RNN(inputDim, hiddenDim )\n",
    "\t\tself.outputLayer=nn.Linear(hiddenDim, outputDim)\n",
    "\t\tself.sigmoid=nn.Sigmoid()\n",
    "\tdef forward(self, x ):\n",
    "\t\t#size of x is T x B x featDim\n",
    "\t\t#B=1 is dummy batch dimension added, because pytorch mandates it\n",
    "\t\t#if you want B as first dimension of x then specift batchFirst=True when LSTM is initalized\n",
    "\t\t#T,D  = x.size(0), x.size(1)\n",
    "\t\t#batch is a must \n",
    "\t\tlstmOut,_ =self.lstm(x ) #x has two  dimensions  seqLen *batch* FeatDim=2\n",
    "\t\tT,B,D  = lstmOut.size(0),lstmOut.size(1) , lstmOut.size(2)\n",
    "\t\tlstmOut = lstmOut.contiguous() \n",
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
    "        # before  feeding to linear layer we squash one dimension\n",
    "        lstmOut = lstmOut.view(L*B,D)\n",
    "        logProb = self.outputLayer(lstmOut) # project lstm states to output probabilities\n",
    "        # reshape actiavtions to T*B*outputlayersize\n",
    "        logProb = logProb.view(L,B,-1).squeeze(1)\n",
    "        return logProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traning the network ###\n",
    "\n",
    "- batch learning is not used, only one seqeuence is fed at a time\n",
    "- runs purely on a cpu\n",
    "- Cross-entropy loss is used"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
   "metadata": {},
=======
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n",
      " Avg. Loss for last 500 samples = inf\n",
<<<<<<< HEAD
      " Avg. Loss for last 500 samples = 0.628634\n",
      " Avg. Loss for last 500 samples = 0.097328\n",
      " Avg. Loss for last 500 samples = 0.002122\n",
      " Avg. Loss for last 500 samples = 0.000786\n",
      " Avg. Loss for last 500 samples = 0.000411\n",
      " Avg. Loss for last 500 samples = 0.000226\n",
      " Avg. Loss for last 500 samples = 0.000138\n",
      " Avg. Loss for last 500 samples = 0.000085\n",
      " Avg. Loss for last 500 samples = 0.000058\n",
      " Avg. Loss for last 500 samples = 0.000041\n",
      " Avg. Loss for last 500 samples = 0.000029\n",
      " Avg. Loss for last 500 samples = 0.000021\n",
      " Avg. Loss for last 500 samples = 0.000015\n",
      " Avg. Loss for last 500 samples = 0.000011\n",
      " Avg. Loss for last 500 samples = 0.000008\n",
      " Avg. Loss for last 500 samples = 0.000006\n",
      " Avg. Loss for last 500 samples = 0.000005\n",
      " Avg. Loss for last 500 samples = 0.000004\n",
      " Avg. Loss for last 500 samples = 0.000003\n",
      " Avg. Loss for last 500 samples = 0.000002\n",
      " Avg. Loss for last 500 samples = 0.000002\n",
      " Avg. Loss for last 500 samples = 0.000001\n"
=======
      " Avg. Loss for last 500 samples = 0.183431\n",
      " Avg. Loss for last 500 samples = 0.175901\n",
      " Avg. Loss for last 500 samples = 0.173191\n",
      " Avg. Loss for last 500 samples = 0.171829\n",
      " Avg. Loss for last 500 samples = 0.165327\n",
      " Avg. Loss for last 500 samples = 0.173916\n",
      " Avg. Loss for last 500 samples = 0.173361\n",
      " Avg. Loss for last 500 samples = 0.168028\n",
      " Avg. Loss for last 500 samples = 0.167956\n",
      " Avg. Loss for last 500 samples = 0.166152\n",
      " Avg. Loss for last 500 samples = 0.160349\n",
      " Avg. Loss for last 500 samples = 0.157556\n",
      " Avg. Loss for last 500 samples = 0.148872\n",
      " Avg. Loss for last 500 samples = 0.141928\n",
      " Avg. Loss for last 500 samples = 0.133639\n",
      " Avg. Loss for last 500 samples = 0.118589\n",
      " Avg. Loss for last 500 samples = 0.104341\n",
      " Avg. Loss for last 500 samples = 0.083448\n",
      " Avg. Loss for last 500 samples = 0.062745\n",
      " Avg. Loss for last 500 samples = 0.048126\n",
      " Avg. Loss for last 500 samples = 0.034521\n",
      " Avg. Loss for last 500 samples = 0.023812\n",
      " Avg. Loss for last 500 samples = 0.016444\n",
      " Avg. Loss for last 500 samples = 0.011685\n",
      " Avg. Loss for last 500 samples = 0.007691\n",
      " Avg. Loss for last 500 samples = 0.005278\n",
      " Avg. Loss for last 500 samples = 0.003757\n",
      " Avg. Loss for last 500 samples = 0.002603\n",
      " Avg. Loss for last 500 samples = 0.001906\n",
      " Avg. Loss for last 500 samples = 0.001378\n",
      " Avg. Loss for last 500 samples = 0.001005\n",
      " Avg. Loss for last 500 samples = 0.000746\n",
      " Avg. Loss for last 500 samples = 0.000536\n",
      " Avg. Loss for last 500 samples = 0.000418\n",
      " Avg. Loss for last 500 samples = 0.000312\n",
      " Avg. Loss for last 500 samples = 0.000237\n",
      " Avg. Loss for last 500 samples = 0.000181\n",
      " Avg. Loss for last 500 samples = 0.000135\n",
      " Avg. Loss for last 500 samples = 0.000103\n"
>>>>>>> 5e57ac5b934be4bccb48a271fbf3f3eb423a17de
     ]
    }
   ],
   "source": [
    "# set here the size of the RNN state:\n",
    "stateSize = 100\n",
    "\n",
    "# set here the size of the binary strings to be used for training:\n",
    "stringLen = 10\n",
    "\n",
    "# create the model:\n",
    "model = Adder(stateSize)\n",
    "print ('model initialized')\n",
    "\n",
    "# create the loss-function:\n",
    "lossFunction = nn.CrossEntropyLoss() #nn.MSELoss()\n",
    "\n",
    "# uncomment below to change the optimizers:\n",
    "#optimizer = optim.SGD(model.parameters(), lr=3e-2, momentum=0.8)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "iterations = 500\n",
    "\n",
    "totalLoss= float(\"inf\")\n",
    "while totalLoss > 1e-6:\n",
    "    print(\" Avg. Loss for last 500 samples = %lf\"%(totalLoss))\n",
    "    totalLoss = 0\n",
    "    for i in range(0,iterations):\n",
    "        # get a new random training sample:\n",
    "        x,y = getSample(stringLen)\n",
    "        # zero the gradients from the previous time-step:\n",
    "        model.zero_grad()\n",
    "        #convert to torch tensor and variable:\n",
    "        ## unsqueeze() is used to add the extra BATCH dimension:\n",
    "        x_var = autograd.Variable(torch.from_numpy(x).unsqueeze(1).float()) \n",
    "        seqLen = x_var.size(0)\n",
    "        x_var = x_var.contiguous()\n",
    "        y_var = autograd.Variable(torch.from_numpy(y))\n",
    "        # push the inputs through the RNN (this is the forward pass):\n",
    "        logProb = model(x_var)\n",
    "        # compute the loss:\n",
    "        loss = lossFunction(logProb,y_var)\n",
    "        totalLoss += loss.data[0]\n",
    "        optimizer.zero_grad()\n",
    "        # perform the backward pass:\n",
    "        loss.backward()\n",
    "        # update the weights:\n",
    "        optimizer.step()\n",
    "    totalLoss=totalLoss/iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model ###\n",
    "Remember that the network was purely trained on strings of length =3 <br>\n",
    "now lets the net on bitstrings of length=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input numbers and their sum  are 863   811   1674\n",
      "binary strings are 1101011111   1100101011   11010001010\n",
      "sum predicted by RNN is  [1 1 0 1 0 0 0 1 0 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 551   673   1224\n",
      "binary strings are 1000100111   1010100001   10011001000\n",
      "sum predicted by RNN is  [1 0 0 1 1 0 0 1 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 894   606   1500\n",
      "binary strings are 1101111110   1001011110   10111011100\n",
      "sum predicted by RNN is  [1 0 1 1 1 0 1 1 1 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 803   896   1699\n",
      "binary strings are 1100100011   1110000000   11010100011\n",
      "sum predicted by RNN is  [1 1 0 1 0 1 0 0 0 1 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 833   647   1480\n",
      "binary strings are 1101000001   1010000111   10111001000\n",
      "sum predicted by RNN is  [1 0 1 1 1 0 0 1 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 608   632   1240\n",
      "binary strings are 1001100000   1001111000   10011011000\n",
      "sum predicted by RNN is  [1 0 0 1 1 0 1 1 0 0 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 522   1000   1522\n",
      "binary strings are 1000001010   1111101000   10111110010\n",
      "sum predicted by RNN is  [1 0 1 1 1 1 1 0 0 1 0]\n",
      "##################################################\n",
      "input numbers and their sum  are 749   796   1545\n",
      "binary strings are 1011101101   1100011100   11000001001\n",
      "sum predicted by RNN is  [1 1 0 0 0 0 0 1 0 0 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 704   611   1315\n",
      "binary strings are 1011000000   1001100011   10100100011\n",
      "sum predicted by RNN is  [1 0 1 0 0 1 0 0 0 1 1]\n",
      "##################################################\n",
      "input numbers and their sum  are 635   951   1586\n",
      "binary strings are 1001111011   1110110111   11000110010\n",
      "sum predicted by RNN is  [1 1 0 0 0 1 1 0 0 1 0]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "stringLen = 10\n",
    "testFlag = 1\n",
    "for i in range (0,10):\n",
    "    x,y = getSample(stringLen,testFlag)\n",
    "    x_var = autograd.Variable(torch.from_numpy(x).unsqueeze(1).float())\n",
    "    y_var = autograd.Variable(torch.from_numpy(y).float())\n",
    "    seqLen = x_var.size(0)\n",
    "    x_var = x_var.contiguous()\n",
    "    finalScores = model(x_var).data.t().numpy()\n",
    "    bits = np.argmax(finalScores,axis=0)\n",
    "    \n",
    "    print ('sum predicted by RNN is ',bits[::-1])\n",
    "    print('##################################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
