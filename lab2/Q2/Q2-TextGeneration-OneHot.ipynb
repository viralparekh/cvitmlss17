{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an RNN  learn to Generate English Text ###\n",
    "- an RNN  is trained in seq2seq manner to make it learn to generate text\n",
    "- with lots of text fed to the network it models the language\n",
    "- The text corpus is split into chunks of fixed length \n",
    "- Each character is represented using a correspodning one hot vector\n",
    "- it learns to model the conditional probability of having a character as next character, given its previous N characters\n",
    "- This code does the unrolling of RNN explicitly using a for loop, to demosntrate how hidden state (output of hidden layer) is carrried forward to the next time-step \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "Throughout this walkthrough, some notations are used for variable names inorder to make code concise. Please get familiar with the notations before you start.\n",
    "\n",
    "The model we're looking to obtain is `g(x)`, and what we obtain after training is `f(x)`. `x` is most obviously input to the function we're looking to learn.\n",
    "\n",
    "We have:\n",
    "\n",
    "```python\n",
    "sequences, predictions, targets = x, y, z = x, f(x), g(x)\n",
    "hidden state = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from random import randint, shuffle\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False # uncomment this if you want to run on CPU alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "We use one-hot vectors to represent each character. This is like a bitstring and for each character we obtain a vector with a single one and remaining zeroes masking using the index of the specific character in the alphabet we want to use.\n",
    "\n",
    "One hot vectors are a rather *sparse* representation, which help us blow up the feature space from something of a lesser dimension. In place of of a one hot vector, you can also use an `Embedding` layer, which takes in a real value (index of char in the alphabet) and learns a *dense* representation of the input space during training, this may bring closer characters together in the resulting feature space.\n",
    "\n",
    "We use the below class to store the alphabet and generate mappings to encode from `char -> onehot` and `onehot -> char`.\n",
    "\n",
    "```haskell\n",
    "OneHotEmbedding.Init :: alphabet -> None\n",
    "OneHotEmbedding.N :: None -> Int\n",
    "OneHotEmbedding.encode :: Char -> onehot vector\n",
    "OneHotEmbedding.label :: Char -> Int\n",
    "OneHotEmbedding.inverse_label :: Int -> Char\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to make one hot embedding when the alphabet is provided \n",
    "#alphabet is the set of uniq characters in your language\n",
    "class OneHotEmbedding:\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "        self.inverse_map = dict(enumerate(alphabet))\n",
    "        self.map = dict(zip(alphabet, range(len(alphabet))))\n",
    "\n",
    "    def N(self):\n",
    "        return len(alphabet)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # T x B x H = len(x) x 1 x N\n",
    "        v = torch.FloatTensor(self.N()).zero_()\n",
    "        v[self.map[x]] = 1\n",
    "        return v\n",
    "    \n",
    "    def label(self, x):\n",
    "        return torch.LongTensor([self.map[x]])\n",
    "\n",
    "    def inverse_label(self, x):\n",
    "        return self.inverse_map[x]\n",
    "\n",
    "    def decode(self, y):\n",
    "        _, max_probs = torch.max(y.transpose(0, 1), 2)\n",
    "        max_probs = max_probs.squeeze()\n",
    "        return self.inverse_classes(max_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling language modelling as a sequence to sequence learning ###\n",
    "![char-rnn seq2seq](charrnn.png)\n",
    "\n",
    "\n",
    "- Input and Target sequences are sequences of characters one shifted in postion\n",
    "- For example if your corpus is \"cvit summer school\" and your chunk_len=4,\n",
    "    - then the first chunk =\"cvit\" . \n",
    "    - Input sequence will be \"cvi\" and \n",
    "    - target is \"vit\"\n",
    "- We then convert each character in your input and target sequence of characters to one hot vectors. Eeach one hot vector will be of size= your alphabet size\n",
    "- From the input sequence and target sequence, we take a pair of input and target and one hot vectors and feed to the network. \n",
    "    - At each instance we calculate the loss\n",
    "    - Once all the timesteps are processed, sum of losses is calculated\n",
    "    - Now we backpropagate the error \n",
    "    \n",
    ".\n",
    "\n",
    "```haskell\n",
    "Network.forward :: x(t), h(t-1) -> y(t), h(t)\n",
    "```\n",
    "\n",
    "Inorder to better understand by manipulating the hidden states, we're building the module so that we can see the hidden state being used explicitly. \n",
    "\n",
    "We're using a `GRU`, you can substitute it with an `RNN` or an `LSTM`, with the required parameters. For an `LSTM`, you'll have to additionally manipulate the cell state in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Def\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, **kw):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = kw['input_size']\n",
    "        self.hidden_size = kw['hidden_size']\n",
    "        self.output_size = kw['output_size']\n",
    "        self.n_layers = kw['n_layers']\n",
    "\n",
    "        self.fc_in = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.rnn = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers)\n",
    "        self.fc_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # One hot vector of single column coming in. \n",
    "        # View sorcery is to adjust to the layer's dimension requirement\n",
    "        # Size(D) -> Size(1,D)\n",
    "\n",
    "        x = self.fc_in(x.view(1, -1))\n",
    "\n",
    "        # Mimicking TxBxD, required by RNN.\n",
    "        # h(t-1) in, h(t) out.\n",
    "        x, h = self.rnn(x.view(1, 1, -1), h)\n",
    "\n",
    "        x = self.fc_out(x.view(1, -1))\n",
    "        return x, h\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "We prepare the data, generate the alphabet from the data and use it to initialize the `OneHotEmbedding` and `Network`.\n",
    "Almost all hyperparameters are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printable=string.printable\n",
    "#reads the text, make everything lower ( so that we will have lower class labels)\n",
    "# and removes non printable characters from the corpus\n",
    "text = open(\"../../../data/lab2/sh.txt\").read().lower()\n",
    "\n",
    "pruned_text = ''\n",
    "for c in text:\n",
    "    if c in printable and c not in '{}[]&_':\n",
    "        pruned_text += c\n",
    "text = pruned_text\n",
    "alphabet = list(set(list(text)))\n",
    "\n",
    "print ('size of your alphabet =', len(alphabet))\n",
    "print ('your alphabet is =', alphabet)\n",
    "\n",
    "onehot = OneHotEmbedding(alphabet)\n",
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## this snippet is needded if only youi want to train the model\n",
    "batch_length = 64\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "#input and output sizes are =len(alphabet) = onehot.N(). \n",
    "net = Network(input_size=onehot.N(), hidden_size=hidden_size, output_size=onehot.N(), n_layers=n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 5e-3\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "if use_cuda:\n",
    "    net=net.cuda()\n",
    "    criterion=criterion.cuda()\n",
    "epoch = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "The following function samples the output from a distribution and helps in predicting new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(**kw):\n",
    "    result = kw['prime']\n",
    "\n",
    "    h = net.init_hidden()\n",
    "    if use_cuda:\n",
    "        h = h.cuda()\n",
    "\n",
    "    x = None\n",
    "\n",
    "    for char in result:\n",
    "        x = onehot.encode(char)\n",
    "        if use_cuda:\n",
    "            x=x.cuda()\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        y, h = net(x, h)\n",
    "\n",
    "        \n",
    "    for p in range(kw[\"length\"]):\n",
    "        y, h = net(x, h)\n",
    "        y_dist = y.data.view(-1).div(kw[\"temperature\"]).exp()\n",
    "        argmax = torch.multinomial(y_dist, 1)[0]\n",
    "\n",
    "        prediction = onehot.inverse_label(argmax)\n",
    "        result += prediction\n",
    "        x = onehot.encode(prediction)\n",
    "        if use_cuda:\n",
    "            x=x.cuda()\n",
    "        x = Variable(x, requires_grad=False)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "We train one *chunk* at a time, which denotes a chunk of a text of length `chunk_size`. \n",
    "\n",
    "For example, consider the text **I've been carrying this thing since 2008.** *(Happy Hogan,Spiderman Homecoming, 2017)*, and a chunk length of `3`. The training loop, if you unroll looks as follows:\n",
    "\n",
    "```python\n",
    "# <I'v>, <'ve> = x, z -------\n",
    "\n",
    "\"'\", h_1 = net(\"I\", h_0)\n",
    "\"v\", h_2 = net(\"'\", h_1)\n",
    "\"e\", h_3 = net(\"v\", h_2)\n",
    "\n",
    "# <e b>, < be> = x, z -------\n",
    "\n",
    "\" \", h_4 = net(\"e\", h_3)\n",
    "\"b\", h_5 = net(\" \", h_4)\n",
    "\"e\", h_6 = net(\"b\", h_5)\n",
    "\n",
    "```\n",
    "\n",
    "So, if you notice the hidden state is carried throughout the text. And as the training progresses, it picks up more and more patterns in the language and is able to generate new text, by sampling from the output distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(100):\n",
    "        # Hidden Layer Initialized only at start.\n",
    "        # Needs to be carried throughout the text.\n",
    "        h = net.init_hidden()\n",
    "        if use_cuda:\n",
    "            h = h.cuda()\n",
    "        for k, i in enumerate(range(0, len(text)-chunk_size, chunk_size)):\n",
    "            chunk = text[i:i+chunk_size+1]\n",
    "            xs, zs = chunk[:-1], chunk[1:]\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            net.zero_grad()\n",
    "            # Iterate through each character -> next character mapping\n",
    "            # Carrying hidden state forward.\n",
    "            for x, z in zip(xs, zs):\n",
    "                x = onehot.encode(x)\n",
    "                z = onehot.label(z)\n",
    "                if use_cuda:\n",
    "                    x = x.cuda()\n",
    "                    z = z.cuda()\n",
    "                x = Variable(x, requires_grad=False)\n",
    "                z = Variable(z)\n",
    "                y, h = net(x, h)\n",
    "                loss += criterion(y.view(1, -1), z)\n",
    "            \n",
    "\n",
    "\n",
    "            # Saving h again, so it's not consumed by .backward() ahead.\n",
    "            h = h.data\n",
    "            if use_cuda:\n",
    "                h = h.cuda()\n",
    "            h = Variable(h, requires_grad=True)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print(\"Loss: \", loss.data[0]/len(xs))\n",
    "            if k % 50 == 0:\n",
    "                new = generate(prime='elementary my dear watson'.lower(), temperature=0.8, length=100)\n",
    "                print(\"----- Generated %d: --------------\\n\"%(k), new)\n",
    "            if k%5000 ==0:\n",
    "                kstring=str(k)\n",
    "                jstring=str(j)\n",
    "                #torch.save(net, 'char_rnn_stateful_onehot_'+jstring+'_'+kstring+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## loading a pretrained model #\n",
    "use_cuda=True\n",
    "del net\n",
    "net=torch.load('../../../data/lab2/char_rnn_stateful_onehot_0_25000.pt')\n",
    "\n",
    "\n",
    "#### DO NOT LOAD THE MODEL MULTIPLE TIMES, IT WILL THROW A CUDA DEVICE ERROR\n",
    "#IF YOU ENCOUNTER SUCH AN ERROR YOU MUST RESTART THE KERNEL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now test on this model ####\n",
    "generatedText= generate(prime='elementary my dear watson'.lower(), temperature=0.1, length=200)\n",
    "print (generatedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ###\n",
    "1. Whats the dimension(tensor.size()) of the variable 'h' in the above code snippet. ? Why is it fed to the network (<i>net(x, h)</i>), along with the the input( x in our case). [hint:- Notice that the network also returns an h everytime]\n",
    "2. For how long does the hidden state is carried forward during training. \n",
    "    - A. it is carried forward from one time step to another, within a sequence. But not from last time step in a sequence to the first timestep of the next sequencce\n",
    "    - B. Not just across time steps within a sequence it is carried forward from one sequence to another\n",
    "    - C. It is carried forward all throughout the training. \n",
    "3. For what value of T is the sampling equivalent to doing an argmax (or picking the most probable label) sampling\n",
    "4. Vary the value of T and see how the text generated varies\n",
    "5. While using the saved model, find out the most proable character the network would predict if your input seed is \"holme\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2###\n",
    "\n",
    "1. In the above code the learning is modelled as seq2seq problem. Your input is a sequence of characters and target is another sequence of characters. Which essentially means you have a target at each time step of the sequence. But this problem of text generation can also be modelled as a sequence to one problem. Then input would be sequence and target is just the next_char in the sequence. Can you modify the code to do this? ( Remember that since it is sequence to one, the output of the hidden layer need to be fed to the output layer only at the last time step)\n",
    "2. We were using 'non over lapping chunks' in the above case. For example if first chunk 1234 then next one was 5678 and so on. How would the results differ if we take  overlapping chunks like 1234 is the fist chunk next is 2345 ?\n",
    "3. Also can you modify the code in such a way that the hidden state is not retained from chunk to another ?\n",
    "4. Try using MSE loss for the above problem. How does the network converge with an MSE loss? Why did MSE perfrom poorer or better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
