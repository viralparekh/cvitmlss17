{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an RNN  learn to Generate English Text ###\n",
    "- an RNN  is trained in seq2seq manner to make it learn to generate text\n",
    "- with lots of text fed to the network it models the language\n",
    "- The text corpus is split into chunks of fixed length \n",
    "- Each character is represented using an index\n",
    "- it learns to model the conditional probability of having a character as next character, given its previous N characters\n",
    "- This code does the unrolling of RNN explicitly using a for loop, to demosntrate how hidden state (output of hidden layer) is carrried forward to the next time-step \n",
    "\n",
    "\n",
    "<b>Acknowledgement :</b>- This code is almost completely copied from here https://gist.github.com/michaelklachko?direction=desc&sort=updated . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN on The Complete Sherlock Holmes.\n",
      "\n",
      "\n",
      "File length: 3867934 characters\n",
      "Unique characters: 52\n",
      "\n",
      "Unique characters: ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "('no of uniq chars', 52)\n"
     ]
    }
   ],
   "source": [
    "printable = string.printable\n",
    " \n",
    "#Input text is available here: https://sherlock-holm.es/stories/plain-text/cano.txt\n",
    "text = open('../../../data/lab2/sh.txt', 'r').read().lower()\n",
    "\n",
    "\n",
    "\n",
    "## remove non printable chars and other unnecessary punctuations\n",
    "pruned_text = ''\n",
    " \n",
    "for c in text:\n",
    "\tif c in printable and c not in '{}[]&_':\n",
    "\t\tpruned_text += c\n",
    " \n",
    "text = pruned_text\t\t  \n",
    "file_len = len(text)\n",
    "alphabet = sorted(list(set(text)))\n",
    "n_chars = len(alphabet)\n",
    "\n",
    "print \"\\nTraining RNN on The Complete Sherlock Holmes.\\n\"\t\t \n",
    "print \"\\nFile length: {:d} characters\\nUnique characters: {:d}\".format(file_len, n_chars)\n",
    "print \"\\nUnique characters:\", alphabet\t\t \n",
    "print ('no of uniq chars', n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_chunk():\n",
    "    start = random.randint(0, file_len - chunk_len)\n",
    "    end = start + chunk_len + 1\n",
    "    return text[start:end]\n",
    "\n",
    "def chunk_vector(chunk):\n",
    "    vector = torch.zeros(len(chunk)).long()\n",
    "    for i, c in enumerate(chunk):\n",
    "        vector[i] = alphabet.index(c)  #construct ASCII vector for chunk, one number per character\n",
    "    return Variable(vector.cuda(), requires_grad=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_training_batch():\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    #construct list of input vectors (chunk_len):\n",
    "    for b in range(batch_size):    \n",
    "        chunk = random_chunk()\n",
    "        inp = chunk_vector(chunk[:-1])\n",
    "        target = chunk_vector(chunk[1:])\n",
    "        inputs.append(inp)\n",
    "        targets.append(target)\n",
    "    #construct batches from lists (chunk_len, batch_size):\n",
    "    #need .view to handle batch_size=1\n",
    "    #need .contiguous to allow .view later\n",
    "    inp = torch.cat(inputs, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "    target = torch.cat(targets, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling language modelling as a sequence to sequence learning ###\n",
    "![char-rnn seq2seq](charrnnembed.png)\n",
    "\n",
    "\n",
    "- Input and Target sequences are sequences of characters one shifted in postion\n",
    "- For example if your corpus is \"cvit summer school\" and your chunk_len=4,\n",
    "    - then the first chunk =\"cvit\" . \n",
    "    - Input sequence will be \"cvi\" and \n",
    "    - target is \"vit\"\n",
    " \n",
    "    \n",
    ".\n",
    "\n",
    "```haskell\n",
    "Network.forward :: x(t), h(t-1) -> y(t), h(t)\n",
    "```\n",
    "\n",
    "Inorder to better understand by manipulating the hidden states, we're building the module so that we can see the hidden state being used explicitly. \n",
    "\n",
    "We're using a `GRU`, you can substitute it with an `RNN` or an `LSTM`, with the required parameters. For an `LSTM`, you'll have to additionally manipulate the cell state in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size) #first arg is dictionary size\n",
    "        self.GRU = nn.GRU(hidden_size, hidden_size, n_layers)  #(input_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, batch_size):\n",
    "        #expand input vector length from single number to hidden_size vector\n",
    "        #Input: LongTensor (batch, seq_len)\n",
    "        #Output: (batch, seq_len, hidden_size)\n",
    "        input = self.encoder(input.view(batch_size, seq_len)) \n",
    "        #need to reshape Input to (seq_len, batch, hidden_size)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        #Hidden (num_layers * num_directions, batch, hidden_size), num_directions = 2 for BiRNN\n",
    "        #Output (seq_len, batch, hidden_size * num_directions)\n",
    "        output, hidden = self.GRU(input, hidden) \n",
    "        #output, hidden = self.GRU(input.view(seq_len, batch_size, hidden_size), hidden) \n",
    "        #Output becomes (batch, hidden_size * num_directions), seq_len=1 (single char)\n",
    "        output = self.decoder(output.view(batch_size, hidden_size))  \n",
    "        #now the output is (batch_size, output_size)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        #Hidden (num_layers * num_directions, batch, hidden_size), num_directions = 2 for BiRNN\n",
    "        return Variable(torch.randn(self.n_layers, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model parameters:\n",
      "\n",
      "n_batches: 200\n",
      "batch_size: 16\n",
      "chunk_len: 128\n",
      "hidden_size: 256\n",
      "n_layers: 2\n",
      "LR: 0.0050\n",
      "\n",
      "\n",
      "Random chunk of text:\n",
      "\n",
      "nce of a\n",
      "     solitary swan. holmes gazed at it and then passed on to the lodge\n",
      "     gate. there he scribbled a short note for st \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTake input, target pairs of chunks (target is shifted forward by a single character)\\nconvert them into chunk vectors\\nfor each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\\nfeed input char vectors to GRU model, and compute error = output - target\\nupdate weights after going through all chars in the chunk\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seq_len = 1        #each character is encoded as a single integer\n",
    "chunk_len = 128    #number of characters in a single text sample\n",
    "batch_size = 16   #number of text samples in a batch\n",
    "n_batches = 200   #size of training dataset (total number of batches)\n",
    "hidden_size = 256  #width of model\n",
    "n_layers = 2      #depth of model\n",
    "LR = 0.005         #learning rate\n",
    "\n",
    "#net = RNN(n_chars, hidden_size, n_chars, n_layers).cuda()\n",
    "net = RNN(n_chars, hidden_size, n_chars, n_layers).cuda()\n",
    "optim = torch.optim.Adam(net.parameters(), LR)\n",
    "cost = nn.CrossEntropyLoss().cuda()  \n",
    "\n",
    "print \"\\nModel parameters:\\n\"\n",
    "print \"n_batches: {:d}\\nbatch_size: {:d}\\nchunk_len: {:d}\\nhidden_size: {:d}\\nn_layers: {:d}\\nLR: {:.4f}\\n\".format(n_batches, batch_size, chunk_len, hidden_size, n_layers, LR)\n",
    "print \"\\nRandom chunk of text:\\n\\n\", random_chunk(), '\\n'\n",
    "    \n",
    "\"\"\"\n",
    "Take input, target pairs of chunks (target is shifted forward by a single character)\n",
    "convert them into chunk vectors\n",
    "for each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\n",
    "feed input char vectors to GRU model, and compute error = output - target\n",
    "update weights after going through all chars in the chunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str = 'a', predict_len = 100, temp = 0.8, batch_size = 1):\n",
    "    hidden = net.init_hidden(batch_size) \n",
    "    prime_input = chunk_vector(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    for i in range(len(prime_str)-1):\n",
    "        _, hidden = net(prime_input[i], hidden, batch_size)\n",
    "     \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for i in range(predict_len):\n",
    "        output, hidden = net(inp, hidden, batch_size)\n",
    "        output_dist = output.data.view(-1).div(temp).exp()  \n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        predicted_char = alphabet[top_i]\n",
    "        predicted +=  predicted_char\n",
    "        inp = chunk_vector(predicted_char)\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "what sleft with before\n",
      "     having on me and you all-comment a more holme, and the room to cab strange \n",
      "\n",
      "[0m 1s (0 / 200) loss: 1.5013]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2060721c5216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#compute output, error, loss:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#calculate gradients, update weights:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-30f8d74c14ab>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden, batch_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#output, hidden = self.GRU(input.view(seq_len, batch_size, hidden_size), hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#Output becomes (batch, hidden_size * num_directions), seq_len=1 (single char)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m#now the output is (batch_size, output_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/minesh.mathew/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mcreator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcreator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mwrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "start = time.time()\n",
    "\n",
    "training_set = []\n",
    "\n",
    "for i in range(n_batches):\n",
    "    training_set.append((random_training_batch()))\n",
    "\n",
    "i = 0    \n",
    "for inp, target in training_set:\n",
    "    #re-init hidden outputs, zero grads, zero loss:\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    net.zero_grad()\n",
    "    loss = 0        \n",
    "    #for each char in a chunk:\n",
    "    #compute output, error, loss:\n",
    "    for c, t in zip(inp, target):\n",
    "        output, hidden = net(c, hidden, batch_size)\n",
    "        loss += cost(output, t)\n",
    "    #calculate gradients, update weights:\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print \"\\n\\nSample output:\\n\"\n",
    "        print evaluate('wh', 100, 0.8), '\\n'\n",
    "        print('[%s (%d / %d) loss: %.4f]' % (time_since(start), i, n_batches, loss.data[0] / chunk_len))\n",
    "\n",
    "    i += 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ###\n",
    "1. Why do you have to take the hidden state from the network each time and pass it along with the next input\n",
    "2. For how long does the hidden state is carried forward during training. \n",
    "    - A. it is carried forward from one time step to another, within a sequence. But not from last time step in a sequence to the first timestep of the next sequencce\n",
    "    - B. Not just across time steps within a sequence it is carried forward from one sequence to another\n",
    "    - C. It is carried forward all throughout the training. \n",
    "3. For what value of T is the sampling equivalent to doing an argmax (or picking the most probable label) sampling\n",
    "4. Vary the value of T and see how the text generated varies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2###\n",
    "\n",
    "1. In the above code the learning is modelled as seq2seq problem. Your input is a sequence of characters and target is another sequence of characters. Which essentially means you have a target at each time step of the sequence. But this problem of text generation can also be modelled as a sequence to one problem. Then input would be sequence and target is just the next_char in the sequence. Can you modify the code to do this? ( Remember that since it is sequence to one, the output of the hidden layer need to be fed to the output layer only at the last time step)\n",
    "\n",
    "\n",
    "4. Try using MSE loss for the above problem. How does the network converge with an MSE loss? Why did MSE perfrom poorer or better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
