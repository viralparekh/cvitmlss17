{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text using RNNs ###\n",
    "- an RNN  is trained in seq2seq manner to make it learn to generate text\n",
    "- with lots of text fed to the network it models the language\n",
    "- it learns to model the conditional probability of having a character as next character, given its previous N characters\n",
    "- This code does the unrolling of RNN explicitly using a for loop\n",
    "\n",
    "\n",
    "<b>Acknowledgement :</b>- This code is almost completely copied from here https://gist.github.com/michaelklachko?direction=desc&sort=updated . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN on The Complete Sherlock Holmes.\n",
      "\n",
      "\n",
      "File length: 3867934 characters\n",
      "Unique characters: 52\n",
      "\n",
      "Unique characters: ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "('no of uniq chars', 52)\n"
     ]
    }
   ],
   "source": [
    "printable = string.printable\n",
    " \n",
    "#Input text is available here: https://sherlock-holm.es/stories/plain-text/cano.txt\n",
    "text = open('../../../data/lab2/sh.txt', 'r').read().lower()\n",
    "\n",
    "\n",
    "\n",
    "## remove non printable chars and other unnecessary punctuations\n",
    "pruned_text = ''\n",
    " \n",
    "for c in text:\n",
    "\tif c in printable and c not in '{}[]&_':\n",
    "\t\tpruned_text += c\n",
    " \n",
    "text = pruned_text\t\t  \n",
    "file_len = len(text)\n",
    "alphabet = sorted(list(set(text)))\n",
    "n_chars = len(alphabet)\n",
    "\n",
    "print \"\\nTraining RNN on The Complete Sherlock Holmes.\\n\"\t\t \n",
    "print \"\\nFile length: {:d} characters\\nUnique characters: {:d}\".format(file_len, n_chars)\n",
    "print \"\\nUnique characters:\", alphabet\t\t \n",
    "print ('no of uniq chars', n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_chunk():\n",
    "    start = random.randint(0, file_len - chunk_len)\n",
    "    end = start + chunk_len + 1\n",
    "    return text[start:end]\n",
    " \n",
    "def chunk_vector(chunk):\n",
    "    vector = torch.zeros(len(chunk)).long()\n",
    "    for i, c in enumerate(chunk):\n",
    "        vector[i] = alphabet.index(c)  #construct ASCII vector for chunk, one number per character\n",
    "    \n",
    "    if use_cuda:\n",
    "        return Variable(vector.cuda(), requires_grad=False) \n",
    "    else:\n",
    "        \n",
    "         return Variable(vector, requires_grad=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_training_batch():\n",
    "\tinputs = []\n",
    "\ttargets = []\n",
    "\t#construct list of input vectors (chunk_len):\n",
    "\tfor b in range(batch_size):    \n",
    "\t\tchunk = random_chunk()\n",
    "\t\tinp = chunk_vector(chunk[:-1])\n",
    "\t\ttarget = chunk_vector(chunk[1:])\n",
    "\t\tinputs.append(inp)\n",
    "\t\ttargets.append(targets)\n",
    "\t#construct batches from lists (chunk_len, batch_size):\n",
    "\t#need .view to handle batch_size=1\n",
    "\t#need .contiguous to allow .view later\n",
    "\tinp = torch.cat(inputs, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "\ttarget = torch.cat(targets, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "\treturn inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "         \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers \n",
    "        self.batch_size = batch_size\n",
    "         \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size) #first arg is dictionary size\n",
    "        self.GRU = nn.GRU(hidden_size, hidden_size, n_layers)  #(input_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "         \n",
    "    def forward(self, input, hidden, batch_size):\n",
    "\n",
    "        input = self.encoder(input.view(batch_size, seq_len)) \n",
    "        #need to reshape Input to (seq_len, batch, hidden_size)\n",
    "        input = input.permute(1, 0, 2)\n",
    "\n",
    "        output, hidden = self.GRU(input, hidden) \n",
    "\n",
    "        output = self.decoder(output.view(batch_size, hidden_size))  \n",
    "        #now the output is (batch_size, output_size)\n",
    "        return output, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Hidden (num_layers * num_directions, batch, hidden_size), num_directions = 2 for BiRNN\n",
    "        if use_cuda:\n",
    "\n",
    "            return Variable(torch.randn(self.n_layers, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.randn(self.n_layers, batch_size, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model parameters:\n",
      "\n",
      "n_batches: 800\n",
      "batch_size: 64\n",
      "chunk_len: 128\n",
      "hidden_size: 256\n",
      "n_layers: 2\n",
      "LR: 0.0050\n",
      "\n",
      "\n",
      "Random chunk of text:\n",
      "\n",
      "ge may come during the day, though wiggins was despondent\n",
      "     about it last night. i want you to open all notes and telegrams, a \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTake input, target pairs of chunks (target is shifted forward by a single character)\\nconvert them into chunk vectors\\nfor each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\\nfeed input char vectors to GRU model, and compute error = output - target\\nupdate weights after going through all chars in the chunk\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 1\t\t   #each character is encoded as a single integer\n",
    "chunk_len = 128    #number of characters in a single text sample\n",
    "batch_size = 64   #number of text samples in a batch\n",
    "n_batches =800   #size of training dataset (total number of batches)\n",
    "hidden_size = 256  #width of model\n",
    "n_layers = 2\t  #depth of model\n",
    "LR = 0.005\t\t   #learning rate\n",
    " \n",
    "net = RNN(n_chars, hidden_size, n_chars, n_layers)\n",
    "#net = RNN(n_chars, hidden_size, n_chars, n_layers)\n",
    "optim = torch.optim.Adam(net.parameters(), LR)\n",
    "cost = nn.CrossEntropyLoss()  \n",
    "\n",
    "if use_cuda:\n",
    "    net=net.cuda()\n",
    "    cost=cost.cuda()\n",
    "\n",
    "print \"\\nModel parameters:\\n\"\n",
    "print \"n_batches: {:d}\\nbatch_size: {:d}\\nchunk_len: {:d}\\nhidden_size: {:d}\\nn_layers: {:d}\\nLR: {:.4f}\\n\".format(n_batches, batch_size, chunk_len, hidden_size, n_layers, LR)\n",
    "print \"\\nRandom chunk of text:\\n\\n\", random_chunk(), '\\n'\n",
    "\t \n",
    "\"\"\"\n",
    "Take input, target pairs of chunks (target is shifted forward by a single character)\n",
    "convert them into chunk vectors\n",
    "for each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\n",
    "feed input char vectors to GRU model, and compute error = output - target\n",
    "update weights after going through all chars in the chunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str = 'elementary my dear watson', predict_len = 250, temp = 0.8, batch_size = 1):\n",
    "\thidden = net.init_hidden(batch_size) \n",
    "\tprime_input = chunk_vector(prime_str)\n",
    "\tpredicted = prime_str\n",
    "\t \n",
    "\tfor i in range(len(prime_str)-1):\n",
    "\t\t_, hidden = net(prime_input[i], hidden, batch_size)\n",
    "\t  \n",
    "\tinp = prime_input[-1]\n",
    "\t \n",
    "\tfor i in range(predict_len):\n",
    "\t\toutput, hidden = net(inp, hidden, batch_size)\n",
    "\t\toutput_dist = output.data.view(-1).div(temp).exp()\t\n",
    "\t\ttop_i = torch.multinomial(output_dist, 1)[0]\n",
    "\t\t \n",
    "\t\tpredicted_char = alphabet[top_i]\n",
    "\t\tpredicted +=  predicted_char\n",
    "\t\tinp = chunk_vector(predicted_char)\n",
    " \n",
    "\treturn predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are forman light. \"i you to my play of down that who had enock it\n",
      "     courece ades into pilcary somewa \n",
      "\n",
      "[0m 18s (0 / 800) loss: 1.5312]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are. where at the man and his passitiver, and mr. before the reflent and\n",
      "     nlas and rouded that the  \n",
      "\n",
      "[1m 2s (100 / 800) loss: 1.5139]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are did i the cour that the blood itsal firm is\n",
      "     sheat is absomams smile maminative which i suggon\n",
      "\n",
      "\n",
      "[1m 46s (200 / 800) loss: 1.4684]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are and that so was romist upossean, me from the velice--take\n",
      "     play a viste the,\" said her be the t \n",
      "\n",
      "[2m 30s (300 / 800) loss: 1.4987]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are as,\" said your, it was not turn in the guided in\n",
      "     he to show streams. i that the gut the proccu \n",
      "\n",
      "[3m 14s (400 / 800) loss: 1.4965]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are seemed your hariss found and\n",
      "     could seemed your really rithe companion what every so. the\n",
      "      \n",
      "\n",
      "[3m 57s (500 / 800) loss: 1.4685]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are soon just him list\n",
      "     taken his work with any conden to conctor,\" he seemed a bephant\n",
      "     in som \n",
      "\n",
      "[4m 42s (600 / 800) loss: 1.4723]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are and think go a have yournast\n",
      "     spoke into my sings of vately in the way, and not have a maning\n",
      "  \n",
      "\n",
      "[5m 26s (700 / 800) loss: 1.5030]\n",
      "('completed iteration no.=', 0)\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are toren, you\n",
      "     fill been on the peenselet to the points the not fure tine from a confide\n",
      "     from \n",
      "\n",
      "[6m 28s (0 / 800) loss: 1.4325]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are the moor he had night a shon\n",
      "     said, and the of his heard would the concent to stepens than a\n",
      "   \n",
      "\n",
      "[7m 13s (100 / 800) loss: 1.4139]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are dread  one with jesed it is new a who would way. then,\n",
      "     was not chair main would placed out oth \n",
      "\n",
      "[8m 2s (200 / 800) loss: 1.4172]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are me well a little forerson,\n",
      "     the child hatectily the police to the clese an head was her was fri \n",
      "\n",
      "[8m 56s (300 / 800) loss: 1.4284]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are hy to me, and creering your of the tell for to las with his for and we\n",
      "     world sir. i would give \n",
      "\n",
      "[9m 49s (400 / 800) loss: 1.4253]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are drous and carristen in in consure of ganswer was all\n",
      "     we was so howers and horse over good. i h \n",
      "\n",
      "[10m 42s (500 / 800) loss: 1.4205]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are the guardence affaons' med that not was the minual\n",
      "     laughty the old and i have severhat friend  \n",
      "\n",
      "[11m 36s (600 / 800) loss: 1.4137]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are scame--into have not come his im angerched upon the\n",
      "     morning husband about the that conclut, wh \n",
      "\n",
      "[12m 28s (700 / 800) loss: 1.4227]\n",
      "('completed iteration no.=', 1)\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are use?\" he some deepon to my\n",
      "     of the clates had halld two seen whysatic, which is get\n",
      "     mark,  \n",
      "\n",
      "[13m 44s (0 / 800) loss: 1.4296]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are no doubt who i have examinaty. so the past you should\n",
      "     and pass, that his come most smally, mra \n",
      "\n",
      "[14m 30s (100 / 800) loss: 1.4194]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are the scaided, watson patient upon hid seat of his\n",
      "     to way, i soine. became always and with who h \n",
      "\n",
      "[15m 14s (200 / 800) loss: 1.4476]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are so sorigg at his boast himself. he father on then of how\n",
      "     between the mornoman pick nelther fos \n",
      "\n",
      "[15m 59s (300 / 800) loss: 1.4555]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are as i had an ejacking that i have to takenwarmed his hand within i the one\n",
      "     used. the never rais \n",
      "\n",
      "[16m 43s (400 / 800) loss: 1.4566]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are\n",
      "      when for a smather in carresier of a tramself, then, and spence. she all sat the at my belver \n",
      "\n",
      "[17m 26s (500 / 800) loss: 1.4355]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are to poor old really,\n",
      "     themsalion, blarknished it an allow into the pares. \"it was yould this tel \n",
      "\n",
      "[18m 10s (600 / 800) loss: 1.4121]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are partorse of i mr. just on that offace\n",
      "     open in the grounds our buldown see has the prease have  \n",
      "\n",
      "[18m 54s (700 / 800) loss: 1.4575]\n",
      "('completed iteration no.=', 2)\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are\n",
      "     to her towed, if i hard mcmmurromn. and can an in passions of\n",
      "     who holmes round it is dres \n",
      "\n",
      "[19m 56s (0 / 800) loss: 1.3966]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are by my am so. that you care.\n",
      "     it was our she the older walk a sire to\n",
      "     post behin mate. \"the \n",
      "\n",
      "[20m 40s (100 / 800) loss: 1.4149]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are\n",
      "     and how and a securiaal recompan in scrare. what and the own\n",
      "     cloud even to his sawersring \n",
      "\n",
      "[21m 23s (200 / 800) loss: 1.4225]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are amous now, and my charry as\n",
      "     \"then have not refused his simpled off book?\"\n",
      "\n",
      "     \"the house and \n",
      "\n",
      "[22m 8s (300 / 800) loss: 1.4087]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are the want of cases. i case a\n",
      "     envels at 1313s, you so violul of the carrea. shemore your conting \n",
      "\n",
      "[22m 51s (400 / 800) loss: 1.3881]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are holmes; to the offersatters. i have not\n",
      "     then,' day not speciazons, her earning to slilly them  \n",
      "\n",
      "[23m 35s (500 / 800) loss: 1.3881]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are have\n",
      "     sure to holmes have traged inten not never eave geners on by the quest\n",
      "     all to you ha \n",
      "\n",
      "[24m 18s (600 / 800) loss: 1.4292]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are doxing by highout have too could can you have no to his saw on\n",
      "     had not crimed, withough his ho \n",
      "\n",
      "[25m 2s (700 / 800) loss: 1.4336]\n",
      "('completed iteration no.=', 3)\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are these with his-coat of hispor end his for the least as\n",
      "     chinaraling his fine faith to helors. i \n",
      "\n",
      "[26m 5s (0 / 800) loss: 1.4981]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are to reach now a death stopon spokelugrand to be\n",
      "     pensidence, the four many upon mrs. i were the  \n",
      "\n",
      "[26m 48s (100 / 800) loss: 1.4875]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are doce we was the me in the word was\n",
      "     should be a blick heard humullist of course.  adventured. \" \n",
      "\n",
      "[27m 32s (200 / 800) loss: 1.4403]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are not was profess youb the contilret had buzn but the bea that you have go\n",
      "     this great of a great \n",
      "\n",
      "[28m 16s (300 / 800) loss: 1.4472]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are plain the first. it seemed in the monders,\n",
      "     up the possible in the pecume, this was to a furnin \n",
      "\n",
      "[29m 1s (400 / 800) loss: 1.4496]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are that suady thought to the bridge, i dest that i\n",
      "     not the quid seemed of posite his however wart \n",
      "\n",
      "[29m 44s (500 / 800) loss: 1.4192]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "you are on that the rucade, docan\n",
      "     as you twentially the comple of youth and this really behind\n",
      "     wh \n",
      "\n",
      "[30m 28s (600 / 800) loss: 1.4607]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for iter in range(0,10):\n",
    "\ttraining_set = []\n",
    " \n",
    "\tfor i in range(n_batches):\n",
    "\t\ttraining_set.append((random_training_batch()))\n",
    " \n",
    "\ti = 0\t\n",
    "\tfor inp, target in training_set:\n",
    "\t\t#re-init hidden outputs, zero grads, zero loss:\n",
    "\t\thidden = net.init_hidden(batch_size)\n",
    "\t\tnet.zero_grad()\n",
    "\t\tloss = 0\t   \n",
    "\t\t#for each char in a chunk:\n",
    "\t\t#compute output, error, loss:\n",
    "\t\tcount=0\n",
    "\t\tfor c, t in zip(inp, target):\n",
    "\t\t\t#print( 'size of c')\n",
    "\t\t\t#print(c.size())\n",
    "\t\t\tcount=count+1\n",
    "\t\t\toutput, hidden = net(c, hidden, batch_size)\n",
    "\t\t\tloss += cost(output, t)\n",
    "\t\t#calculate gradients, update weights:\n",
    "\t\t#print('count was')\n",
    "\t\t#print (count)\n",
    "\t\tloss.backward()\n",
    "\t\toptim.step()\n",
    " \n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint \"\\n\\nSample output:\\n\"\n",
    "\t\t\tprint evaluate('you are', 100, 0.8), '\\n'\n",
    "\t\t\tprint('[%s (%d / %d) loss: %.4f]' % (time_since(start), i, n_batches, loss.data[0] / chunk_len))\n",
    " \t\t\twith open(\"model_iteration_iter_%d_i_%d.pth\"%(iter, i), \"w+\") as fp:\n",
    "\t\t\t\ttorch.save(net, fp)\n",
    " \n",
    "\t\ti += 1\n",
    "\t#print('i is')\n",
    "\t\n",
    "\t#print (i)\n",
    "\tprint ( 'completed iteration no.=', iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
