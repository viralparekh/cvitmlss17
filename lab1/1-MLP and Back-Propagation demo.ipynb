{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of MLP and Backpropagation on MNIST dataset\n",
    "\n",
    "In this tutorial we will understand how to implement a Multilayer perceptron architecture with one hidden layer. This tutorial has two parts: (a) Implementing Back-propagation from scratch (b) Using the in-built 'Autograd' module to train the MLP network.\n",
    "\n",
    "To make data loading simple, we would use the torchvision package created as part of PyTorch which has data loaders for standard datasets such as ImageNet, CIFAR10, MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "print(\"Importing done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # Batch size\n",
    "input_dim = 784 # Input dimension (For MNIST dataset each image is of size 28 x 28 = 784)\n",
    "num_of_hidden_nodes = 100 # number of hidden nodes in hidden layer\n",
    "output_dim = 10 # Number of output nodes = no of classes in th dataset. In this case it is 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST data. \n",
    "For convenience we have already downloaded the MNIST dataset and saved in the '../../data' folder. So, the argument download is set to 'False'. We then whiten the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../../data', train=True, download=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                           batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation function and its derivative\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "$\\sigma^{'}(x) = \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/torch.exp(x.mul(-1)).add(1)\n",
    "    \n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return torch.mul(sigmoid(x), sigmoid(x).mul(-1).add(1))\n",
    "\n",
    "# tensor = torch.FloatTensor([[1,2,3],[1,2,3]])\n",
    "# print(sigmoid(tensor)) # You can use it for debugging\n",
    "# torch.sigmoid(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weight matrices with some random values\n",
    "\n",
    "$W_1 \\in \\mathbb{R}^{784 x 100}$\n",
    "\n",
    "$W_2 \\in \\mathbb{R}^{100 x 10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiliaze the weights\n",
    "W_1 = torch.randn(input_dim, num_of_hidden_nodes).type(torch.FloatTensor) # Weights between input and hidden layer\n",
    "W_2 = torch.randn(num_of_hidden_nodes, output_dim).type(torch.FloatTensor) # Weights between hidden layer and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop with manual backpropagation\n",
    "\n",
    "In each epoch, we will have several batches of data. We take each of the batches and do the forward pass. Then based on the error we back-propagate.\n",
    "\n",
    "![alt text](images/mlp.png \"MLP with 3-layers\")\n",
    "\n",
    "\n",
    "Assume, batch_size = 1, matrix multiplication $*$ and element-wise multiplication $.$\n",
    "\n",
    "### Mean-Squared Loss Function:\n",
    "\n",
    "$L = 0.5*(output - true\\_output)^2$\n",
    "\n",
    "### Forward Pass:\n",
    "\n",
    "$Z = \\sigma(W_1^{T}X)$           [$\\mathbb{R}^{1 x 100}$]\n",
    "\n",
    "$output = \\sigma(W_2^{T}Z)$       [$\\mathbb{R}^{1 x 10}$]\n",
    "\n",
    "### Backward Pass:\n",
    "\n",
    "Derivative of loss: $diff = (output - true\\_output)$   [$\\mathbb{R}^{1 x 10}$]\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_2} = Z^{T}*(diff.\\sigma^{'}(output))$    [$\\mathbb{R}^{100 x 10}$]\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_1} = X^{T} *((diff.\\sigma^{'}(output))*W_2^{T}).\\sigma^{'}(Z)$ [$\\mathbb{R}^{784 x 100}$]\n",
    "\n",
    "### Parameter Update:\n",
    "\n",
    "$W_1 = W_1 - \\eta \\frac{\\partial L}{\\partial W_1}$\n",
    "\n",
    "$W_2 = W_2 - \\eta \\frac{\\partial L}{\\partial W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 3.5688973818297307 | accuracy: 0.8774833333333333\n",
      "Epoch: 1 | loss: 2.6475644906489237 | accuracy: 0.9040833333333333\n",
      "Epoch: 2 | loss: 2.527936198289725 | accuracy: 0.9094833333333333\n",
      "Epoch: 3 | loss: 2.440844534249204 | accuracy: 0.9124166666666667\n",
      "Epoch: 4 | loss: 2.396329473502439 | accuracy: 0.91395\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    y_batch_onehot = torch.FloatTensor(batch_size, output_dim)\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Forward Pass\n",
    "        x_batch = x_batch.view(-1, 784)\n",
    "        hidden_state_output = sigmoid(torch.mm(x_batch, W_1))\n",
    "        output = sigmoid(torch.mm(hidden_state_output, W_2))\n",
    "        \n",
    "        # Convert the labels to one hot encoded format\n",
    "        y_batch_onehot.zero_()\n",
    "        y_batch_onehot.scatter_(1, y_batch[:, None], 1)\n",
    "        \n",
    "        # Loss (Mean-Squared error)     \n",
    "        loss += (output - y_batch_onehot).pow(2).sum()*0.5\n",
    "        _, predicted_class = output.max(1)\n",
    "        correct += predicted_class.eq(y_batch).sum()       \n",
    "        \n",
    "        #Backward Pass (Back-Propagation)\n",
    "        # Derivative of MSE Loss        \n",
    "        diff = (output - y_batch_onehot)\n",
    "\n",
    "        grad_w2 = torch.mm(hidden_state_output.t(),torch.mul(diff, sigmoid_diff(output))) # 100 x 10 dimensional\n",
    "        grad_w1 =  torch.mm(x_batch.t(),torch.mul(torch.mm(torch.mul(diff, sigmoid_diff(output)), W_2.t())\n",
    "                             ,sigmoid_diff(hidden_state_output))) # 784 x 100\n",
    "        \n",
    "        # Perform gradient descent        \n",
    "        W_1 -= learning_rate*grad_w1\n",
    "        W_2 -= learning_rate*grad_w2\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, loss/len(train_loader)\n",
    "                                                          , correct/float(len(train_loader.dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in-built Autograd function\n",
    "\n",
    "loss.backward():  calculates the gradients of the loss function w.r.t all the parameters in the network\n",
    "\n",
    "optimizer.step(): updates all the parameters of the networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 10.18713915570577 | accuracy: 0.5025166666666666\n",
      "Epoch: 1 | loss: 5.959336371231079 | accuracy: 0.7103833333333334\n",
      "Epoch: 2 | loss: 5.353478724352518 | accuracy: 0.7298833333333333\n",
      "Epoch: 3 | loss: 5.024498683547973 | accuracy: 0.7389166666666667\n",
      "Epoch: 4 | loss: 4.816555805460612 | accuracy: 0.74595\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# import pdb\n",
    "learning_rate = 0.1\n",
    "\n",
    "W_1 = Variable(torch.randn(input_dim, num_of_hidden_nodes).type(torch.FloatTensor).cuda(), requires_grad=True)\n",
    "W_2 = Variable(torch.randn(num_of_hidden_nodes, output_dim).type(torch.FloatTensor).cuda(), requires_grad=True)\n",
    "y_batch_onehot = Variable(torch.FloatTensor(batch_size, output_dim).cuda(), requires_grad=True)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = Variable(x_batch.view(-1,784).cuda(), requires_grad=False)\n",
    "        y_batch = Variable(y_batch.cuda(), requires_grad=False)       \n",
    "        \n",
    "        # Forward Pass\n",
    "        hidden_state_output = torch.sigmoid(torch.mm(x_batch, W_1))\n",
    "        output = torch.sigmoid(torch.mm(hidden_state_output, W_2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Convert the labels to one hot encoded format\n",
    "        y_batch_onehot.data.zero_()\n",
    "        y_batch_onehot.data.scatter_(1, y_batch[:, None].data, 1)\n",
    "\n",
    "        \n",
    "        # Loss (Mean-Squared error)  \n",
    "#         pdb.set_trace()\n",
    "        loss = (output - y_batch_onehot).pow(2).sum().mul(0.5)\n",
    "        total_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "\n",
    "        # Calculate no of correct classifications\n",
    "        _, predicted_class = output.max(1)\n",
    "        correct += predicted_class.data.eq(y_batch.data).sum()              \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        W_1.data -= learning_rate * W_1.grad.data\n",
    "        W_2.data -= learning_rate * W_2.grad.data\n",
    "                 # Manually zero the gradients before running the backward pass         \n",
    "        W_1.grad.data.zero_()\n",
    "        W_2.grad.data.zero_()\n",
    "\n",
    "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, total_loss/len(train_loader)\n",
    "                                                          , correct/float(len(train_loader.dataset))))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
