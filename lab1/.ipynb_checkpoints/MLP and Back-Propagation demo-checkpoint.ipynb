{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "print(\"Importing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # Batch size\n",
    "input_dim = 784 # Input dimension (For MNIST dataset each image is of size 28 x 28 = 784)\n",
    "num_of_hidden_nodes = 100 # number of hidden nodes in hidden layer\n",
    "output_dim = 10 # Number of output nodes = no of classes in th dataset. In this case it is 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../../data', train=True, download=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                           batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/torch.exp(x.mul(-1)).add(1)\n",
    "    \n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return torch.mul(sigmoid(x), sigmoid(x).mul(-1).add(1))\n",
    "\n",
    "# tensor = torch.FloatTensor([[1,2,3],[1,2,3]])\n",
    "# print(sigmoid(tensor)) # You can use it for debugging\n",
    "# torch.sigmoid(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiliaze the weights\n",
    "W_1 = torch.randn(input_dim, num_of_hidden_nodes).type(torch.FloatTensor) # Weights between input and hidden layer\n",
    "W_2 = torch.randn(num_of_hidden_nodes, output_dim).type(torch.FloatTensor) # Weights between hidden layer and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 3.5814903906020006 | accuracy: 0.8759833333333333\n",
      "Epoch: 1 | loss: 2.579263446009155 | accuracy: 0.9038666666666667\n",
      "Epoch: 2 | loss: 2.472711461259164 | accuracy: 0.9091333333333333\n",
      "Epoch: 3 | loss: 2.4186381172526987 | accuracy: 0.9101833333333333\n",
      "Epoch: 4 | loss: 2.3487378365596916 | accuracy: 0.9146166666666666\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    y_batch_onehot = torch.FloatTensor(batch_size, output_dim)\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Forward Pass\n",
    "        x_batch = x_batch.view(-1, 784)\n",
    "        hidden_state_output = sigmoid(torch.mm(x_batch, W_1))\n",
    "        output = sigmoid(torch.mm(hidden_state_output, W_2))\n",
    "        \n",
    "        # Convert the labels to one hot encoded format\n",
    "        y_batch_onehot.zero_()\n",
    "        y_batch_onehot.scatter_(1, y_batch[:, None], 1)\n",
    "        \n",
    "        # Loss (Mean-Squared error)     \n",
    "        loss += (output - y_batch_onehot).pow(2).sum()*0.5\n",
    "        _, predicted_class = output.max(1)\n",
    "        correct += predicted_class.eq(y_batch).sum()       \n",
    "        \n",
    "        #Backward Pass (Back-Propagation)\n",
    "        # Derivative of MSE Loss        \n",
    "        diff = (output - y_batch_onehot)\n",
    "        grad_w2 = torch.mm(hidden_state_output.t(),torch.mul(diff, sigmoid_diff(output))) # 100 x 10 dimensional\n",
    "        grad_w1 =  torch.mm(x_batch.t(),torch.mul(torch.mm(torch.mul(diff, sigmoid_diff(output)), W_2.t())\n",
    "                             ,sigmoid_diff(hidden_state_output))) # 784 x 100\n",
    "        \n",
    "        # Perform gradient descent        \n",
    "        W_1 -= learning_rate*grad_w1\n",
    "        W_2 -= learning_rate*grad_w2\n",
    "        \n",
    "        \n",
    "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, loss/len(train_loader)\n",
    "                                                          , correct/len(train_loader.dataset)))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 9.341431047058105 | accuracy: 0.5224333333333333\n",
      "Epoch: 1 | loss: 6.628281626129151 | accuracy: 0.6561666666666667\n",
      "Epoch: 2 | loss: 6.187631152788798 | accuracy: 0.6687666666666666\n",
      "Epoch: 3 | loss: 5.620929303741455 | accuracy: 0.7129333333333333\n",
      "Epoch: 4 | loss: 3.653312045923869 | accuracy: 0.8355\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# import pdb\n",
    "learning_rate = 0.1\n",
    "\n",
    "W_1 = Variable(torch.randn(input_dim, num_of_hidden_nodes).type(torch.FloatTensor).cuda(), requires_grad=True)\n",
    "W_2 = Variable(torch.randn(num_of_hidden_nodes, output_dim).type(torch.FloatTensor).cuda(), requires_grad=True)\n",
    "y_batch_onehot = Variable(torch.FloatTensor(batch_size, output_dim).cuda(), requires_grad=True)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = Variable(x_batch.view(-1,784).cuda(), requires_grad=False)\n",
    "        y_batch = Variable(y_batch.cuda(), requires_grad=False)       \n",
    "        \n",
    "        # Forward Pass\n",
    "        hidden_state_output = torch.sigmoid(torch.mm(x_batch, W_1))\n",
    "        output = torch.sigmoid(torch.mm(hidden_state_output, W_2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Convert the labels to one hot encoded format\n",
    "        y_batch_onehot.data.zero_()\n",
    "        y_batch_onehot.data.scatter_(1, y_batch[:, None].data, 1)\n",
    "\n",
    "        \n",
    "        # Loss (Mean-Squared error)  \n",
    "#         pdb.set_trace()\n",
    "        loss = (output - y_batch_onehot).pow(2).sum().mul(0.5)\n",
    "        total_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "\n",
    "        # Calculate no of correct classifications\n",
    "        _, predicted_class = output.max(1)\n",
    "        correct += predicted_class.data.eq(y_batch.data).sum()              \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        W_1.data -= learning_rate * W_1.grad.data\n",
    "        W_2.data -= learning_rate * W_2.grad.data\n",
    "                 # Manually zero the gradients before running the backward pass         \n",
    "        W_1.grad.data.zero_()\n",
    "        W_2.grad.data.zero_()\n",
    "\n",
    "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, total_loss/len(train_loader)\n",
    "                                                          , correct/len(train_loader.dataset)))              \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
