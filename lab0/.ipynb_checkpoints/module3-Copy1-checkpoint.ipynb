{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "A MLP is a network of neurons in which each row of neurons is called a layer and one network can have multiple layers. The layers are:\n",
    "## Input Layer\n",
    "The bottom layer that takes input from your dataset is called the visible layer, because it is the exposed part of the network. Often a neural network is drawn with a visible layer with one neuron per input value or column in your dataset.\n",
    "\n",
    "## Hidden Layer\n",
    "Layers after the input layer are called hidden layers because that are not directly exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.\n",
    "\n",
    "## Output Layer\n",
    "The final hidden layer is called the output layer and it is responsible for outputting a value or vector of values that correspond to the format required for the problem.\n",
    "\n",
    "An example of multiLayer perceptron with a single hidden layer is shown below.The MLP structure is as follows\n",
    "\n",
    " Layer       : Number of Neurons\n",
    "\n",
    " input layer:    2\n",
    "\n",
    " hidden layer:   50\n",
    "\n",
    " ouput layer:     1\n",
    "\n",
    "\n",
    "\n",
    "![title](../data/lab0/mlp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scientific computing library for Python\n",
    "import numpy as np\n",
    "\n",
    "# A Tensor library with GPU support\n",
    "import torch\n",
    "\n",
    "#A neural networks library integrated with autograd functionality\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\t\n",
    "\n",
    "#an optimization package with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.\n",
    "import torch.optim as optim\n",
    "\n",
    "#differentiation library that supports all differentiable Tensor operations in torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#plotting and visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "#Display on the notebook\n",
    "%matplotlib inline \n",
    "plt.ion() #Turn interactive mode on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training data  with 2 features\n",
    "trainingdataX = [[[0.01, 0.01], [0.01, 0.90], [0.90, 0.01], [0.95, 0.95]], [[0.02, 0.03], [0.04, 0.95], [0.97, 0.02], [0.96, 0.95]]]\n",
    "# The ground truth corresponding to each sample data\n",
    "trainingdataY = [[[0.01], [0.90], [0.90], [0.01]], [[0.04], [0.97], [0.98], [0.1]]]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a network, we should first inhert the base class nn.Module. You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 50,bias=False) # 2 Input noses, 10 in middle layers\n",
    "        self.fc2 = nn.Linear(50, 1, bias=False) # 10 middle layer, 1 output nodes\n",
    "        self.rl1 = nn.ReLU()\n",
    "        self.rl2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.rl1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.rl2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## Create Network\n",
    "\n",
    "    net = Net()\n",
    "    print net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the parameter values\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[1].size()) \n",
    "print(params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumEpoches = 200\n",
    "np.arange(0,NumEpoches,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Optimization and Loss\n",
    "\n",
    "#defining the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#defining the optimization function as stochastic gradient descent function\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "#defining the total number of epochs\n",
    "NumEpoches = 200\n",
    "trainLoss = np.zeros((NumEpoches))  \n",
    "#Iterating for the number of epochs with 2 iterations in each epoch\n",
    "for epoch in range(NumEpoches):\n",
    "    \n",
    "    #Initialing the loss as 0\n",
    "    running_loss = 0.0\n",
    "    #printing the epoch number\n",
    "    print \"epoch[%d]\"%epoch\n",
    "    \n",
    "    \n",
    "    #Iterating over the samples in an epoch \n",
    "    #It takes total 2 iterations because total number of samples are 8 and \n",
    "    #in one pass we are passing 4 samples through the network)\n",
    "    \n",
    "    for i, data in enumerate(trainingdataX, 0):\n",
    "        #print i\n",
    "        \n",
    "        # get the inputs\n",
    "        inputs = data\n",
    "        labels = trainingdataY[i]\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(torch.FloatTensor(inputs))\n",
    "        labels = Variable(torch.FloatTensor(labels))\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        print \"loss: \", running_loss\n",
    "    trainLoss[epoch]=running_loss\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Finished training...\"\n",
    "print net(Variable(torch.FloatTensor(trainingdataX[0])))\n",
    "print net(Variable(torch.FloatTensor(trainingdataX[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
