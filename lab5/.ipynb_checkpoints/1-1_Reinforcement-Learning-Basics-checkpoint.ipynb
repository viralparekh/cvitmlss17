{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Basics\n",
    "\n",
    "**Reinforcement learning** (RL) is a category of machine learning and it is best understood as If we have an **agent** that interacts with an **environment** such that it can observe the environment **state** and perform **actions**. Upon doing actions, the environment state changes into a new state and the agent recieves a **reward** (or penalty). Reinforcement learning aims at making this agent learn from his experience of interactions with environment so that it chooses the best actions that maximizes the sum of rewards it receives from the environment. \n",
    "<img src=\"images/rl_diagram.png\">\n",
    "\n",
    "Mathematically, RL is described using a framework called **Markov Decision Process** (MDP). Formally, MDP is defined by a tuple of five items $<S, A, P, R, $$\\gamma$$>$, which are:\n",
    "- $S$:  Set of observations. The agent observes the environment state as one item of this set.\n",
    "- $A$: Set of actions. The set of actions the agent can choose one from to interact with the environment.\n",
    "- $P$: $P(s'| s, a)$ transition probability matrix. This models what next state $s'$ will be after the agent makes action a while being in the current state $s$.\n",
    "- $R$: $P(r | s, a)$ reward model that models what reward the agent will recieve when it performs action $a$ when it is in state $s$.\n",
    "- $\\gamma$: discount factor. This factor is a numerical value between 0 and 1 that represents the relative importance between immediate and future rewards. \n",
    "\n",
    "The goal of the RL, is to solve the MDP by finding *optimal policy* which means finding the sequence of action it can make to maximize the total recieved reward. The two fundamental method for solving MDP's are **value iteration** and **policy iteration** algorithms. Both value-iteration and policy-iteration assume that the agent knows the MDP model of the world (i.e. the agent knows the state-transition and reward probability functions). Therefore, they can be used by the agent to (offline) plan its actions given knowledge about the environment before interacting with it. We will discuss these two approaches in this notebook. \n",
    "\n",
    "### MDP\n",
    "\n",
    "We saw that MDP consists of a tuple of 5 elements, $<S, A, P, R, $$\\gamma$$>$.  The way by which the agent choses which action to perform is named the agent **policy** which is a function that takes the current environment state to return an action. The policy is often denoted by the symbol $\\pi$.\n",
    "$$\\pi(s) : \\mathbb{S} \\rightarrow \\mathbb{A}$$\n",
    "\n",
    "Let’s now differentiate between two types environments.\n",
    "\n",
    " - **Deterministic environment**: deterministic environment means that both state transition model and reward model are detereminstic functions. If the agent while in a given state repeats a given action, it will always go the same next state and recieve the same reward value.\n",
    " \n",
    "- **Stochastic environment**: In a stochastic environment there is uncertainity about the actions effect. When the agent repeats doing the same action in a given state, the new state and received reward may not be the same each time. For example, a robot which tries to move forward but because of the imperfection in the robot operation or other factors in the environment (e.g. slippery floor), sometimes the action forward will make it move forward but in sometimes, it will move to left or right.\n",
    "\n",
    "Deterministic environments are easier to solve, because the agent knows how to plan its actions with no-uncertaintiy given the environment MDP. Possibly, the environment can be modeled in as a graph where each state is a node and edeges represent transition actions frome one state to another and edge weights are recieved rewards. Then, the agent can use a graph search algorithm such as A* to _find the path with maximum total reward form the initial state_.\n",
    "\n",
    "The goal of the agent is to pick the best policy that will maximize the total rewards recieved from the environment. \n",
    "\n",
    "Assume that environment is intially at state $s_0$. At time 0 : Agent observes the environment state $s_0$ and picks an action $a_0$, then upon performing its action, environment state becomes $s_1$ and the agent recieves a reward $r_1$. At time 1: Agent observes current state $s_1$ and picks an action $a_1$ , then upon acting its action, environment state becomes $s_2$ and it recieves a reward $r_2$. So the **total reward** recieved by the agent in response to the actions selected by its policy is going to be: \n",
    "$$\\text{total_reward} = r_1 + r_2 + \\dots$$.\n",
    "\n",
    "However, it is common to use a discount factor to give higher weight to near rewards recieved near than rewards recieved further in the future.\n",
    "$$\\text{total_discounted_reward} = r_1 + \\gamma\\ r_2 + \\gamma^{2}\\ r_3 + \\dots = \\sum_{i=1}^{T} \\gamma^{i-1}\\ r_i$$\n",
    "where `T` is the horizon (episode length) which can be infinity if there is maximum length for the episode.\n",
    "\n",
    "\n",
    "The **value function** $V(s)$ represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state $s$. The value function depends on the policy $\\pi$ by which the agent picks actions to perform.\n",
    "$$V^{\\pi}(s) = \\mathbb{E}[\\sum_{i=1}^T \\gamma^{i-1} r_i], \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "Among all possible value-functions, there exist an optimal value function that has higher value than other functions for all states.\n",
    "$$V^{*}(s) = \\max_{\\pi} V^{\\pi}(s), \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "The optimal policy $\\pi^{*}$ is the policy that corresponds to optimal value function.\n",
    "$$\\pi^{*} = \\arg\\max_{\\pi} V^{\\pi}(s), \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "In addition to the state value-function, for convenience RL algorithms introduce another function which is the state-action pair **Q function**. Q is a function of a state-action pair and returns a real value.\n",
    "$$\\mathbf{Q}:\\mathbb{S} \\times \\mathbb{A} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "The optimal Q-function $Q^{*}(s, a)$ means the expected total reward recieved by an agent starting in sand picks action $a$, then will behave optimally afterwards. There, $Q^{*}(s, a)$ is an indication for how good it is for an agent to pick action a while being in state $s$.\n",
    "\n",
    "Since $V^{*}(s)$ is the maximum expected total reward when starting from state $a$, it will be the maximum of $Q^{*}(s, a)$over all possible actions. Therefore, the relationship between Q*(s, a) and V*(s) is easily obtained as:\n",
    "$$V^{*}(s) = \\max_{a} Q^{*}(s,a), \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "and If we know the optimal Q-function Q*(s, a) , the optimal policy can be easily extracted by chosing the action a that gives maximum Q*(s, a) for state s.\n",
    "\n",
    "Now, given these, we can introduce **Bellman equations**. Bellman equation using dynamic programming paradigm provides a recursive definition for the optimal Q-function. The $Q^{*}(s, a)$ is equal to the summation of immediate reward after performing action $a$ while in state $s$ and the discounted expected future reward after transition to a next state $s'$.\n",
    "$$Q^{*}(s,a) = R(s,a) + \\gamma \\mathbb{E}_{s'}[V^{*}(s')]$$\n",
    "$$Q^{*}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathbb{S}} p(s'|s,a) V^{*}(s')$$\n",
    "\n",
    "Since $V^{*}(S) = \\max_{a} Q^{*}(s, a)$, we have\n",
    "$$V^{*}(S) = \\max_{a} \\Big( R(s,a) + \\gamma \\sum_{s' \\in \\mathbb{S}} p(s'|s, a)V^{*}(s,a) \\Big)$$\n",
    "\n",
    "Value-iteration and policy iteration rely on these equations to compute the optimal value-function.\n",
    "\n",
    "First, let us take a look at our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction to environment - Frozen Lake\n",
    "\n",
    "For this lab we will solve the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0) environment from [OpenAI gym](https://gym.openai.com/). \n",
    "<img src=\"images/frozen_lake.png\" width=50%>\n",
    "\n",
    "The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole as shown below:\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1.\n",
    "\n",
    "#### Working with OpenAI gym\n",
    "To get you acquanted with OpenAI's gym environment, here are some basic syntax:\n",
    "1. `env.reset()` - resets the environment\n",
    "2. `env.step(action)` - applies the given action in the environment which returns four values:<br>\n",
    "(a) `observation` (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game. <br>\n",
    "(b) `reward` (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. <br>\n",
    "(c) `done` (boolean): whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.) <br>\n",
    "(d) `info` (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Search\n",
    "\n",
    "Now, lets see if how far can a `random search` for a good policy achieve for this problem.\n",
    "In this problem, we have 16 states and 4 possible moves. As a result, there exist $4^{16}=4294967296$ possible policies. Of course, it is not feasible to evaluate all of them, but we can generate a random set of solutions and select the best among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 12:35:45,893] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import gym\n",
    "\n",
    "def run_episode(env, policy, episode_len=100, render=False):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    ## Policy search\n",
    "    n_policies = 2000\n",
    "    start = time.time()\n",
    "    policy_set = [gen_random_policy() for _ in range(n_policies)]\n",
    "    policy_score = [evaluate_policy(env, p) for p in policy_set]\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Best score = %0.2f. Time taken = %4.4f seconds\" %(np.max(policy_score) , end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning with Tables\n",
    "\n",
    "We will now see, how Q-Learning can solve our `frozen lake` problem. In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n",
    "\n",
    "We make updates to our Q-table using something called the Bellman equation (as discussed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "gamma = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "print (\"Score over time: \" +  str(sum(rList)/num_episodes))    \n",
    "print (\"Time taken\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Final Q-Table Values\")\n",
    "print (Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning with Neural Networks\n",
    "\n",
    "Tables are great, but they don’t really scale! While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a **function approximator**, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set learning hyper-parameter value\n",
    "gamma = 0.99\n",
    "e = 0.1\n",
    "mu = 0.07\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Network\n",
    "# W = Variable(torch.normal(0, 0.01), requires_grad=True)\n",
    "# Define loss\n",
    "# def criterion(Q, Q_next):\n",
    "#     return (Q-Q_next).pow(2).sum()\n",
    "\n",
    "# Initialize the net\n",
    "Q = Q_Net()\n",
    "Q_next = Q_Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = optim.SGD(Q.parameters(), lr=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# Training the network\n",
    "start_time = time.time()\n",
    "for i in range(num_episodes):\n",
    "    # reset the environment to get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # The Q-Network\n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        # choose an action greedily (with e chance of random action) from the Q-Network\n",
    "        inp = Variable(torch.eye(16), requires_grad=False)\n",
    "        # convert model and input to cuda objects if GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            Q.cuda()\n",
    "            Q_next.cuda()\n",
    "            inp = inp.cuda()\n",
    "        \n",
    "        # reset the gradient matrix of the network\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        current_Q_value = Q(inp[s:s+1])\n",
    "        # print (allQ)\n",
    "        # take the arg max of the output\n",
    "        _, a = torch.max(current_Q_value.data, 1)\n",
    "\n",
    "        # choose an action greedily (by e chance of random action) from the Q-Network\n",
    "        if (np.random.rand(1) < e):\n",
    "            a[0,0] = env.action_space.sample()\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        s1, r, d, _ = env.step(a[0, 0])\n",
    "        \n",
    "        # Obtain the Q' values by feeding the new state through our network\n",
    "        next_Q_value = Q_next(inp[s1:s1+1]).detach()\n",
    "        \n",
    "        # Obtain maxQ' and set our target value for our chosen function\n",
    "        max_nextQ_value = torch.max(next_Q_value)\n",
    "        \n",
    "        targetQ = current_Q_value.data\n",
    "        targetQ[0, a[0,0]] = r + gamma*max_nextQ_value.data[0]\n",
    "\n",
    "        # Update the parameters of the network\n",
    "        targetQ = Variable(targetQ, requires_grad=False)\n",
    "        \n",
    "        loss = criterion(current_Q_value, next_Q_value)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if (d == True):\n",
    "            # Reduce the chance of random action as we train the model\n",
    "            e = 1.0 / ((i/50) + 10)\n",
    "            break\n",
    "    jList.append(j)\n",
    "    rList.append(rAll)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "print (\"Total time\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
